---
title: "Swiss Public Charging Data Analysis"
output: html_document
date: "2024-11-04"
---

```{r setup, include=FALSE}
library(pxR)
library(dplyr)
library(tidyverse)
library(csv)

```


```{r charging data folder location}
monthly_chg_files <- "C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data"
chunked_chg_files <- "C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1"
```

```{r charging point details - Prior to July 2025. Can DELETE}
CH_EVSE_details <- read.csv("M:/Rudolph/R-Projects/swiss-public-charging/data/EVSE_details_2024-10-04h.csv") %>%
  mutate(Power_kW = as.integer(sub("^(\\d+).*", "\\1", ChargingFacilities)))

CH_EVSE_details_short <- CH_EVSE_details %>%
  select(EvseID, ChargingStationId, ChargingFacilities, Power_kW, Accessibility, City, GeoCoordinates) %>%
  separate(GeoCoordinates, into = c("latitude", "longitude"), sep = " ", convert = TRUE)

```

```{r Charging Point Details}
CH_EVSE_details <- read.csv("M:/Rudolph/R-Projects/swiss-public-charging/data/ChargingStationDetails_22-07-2025.csv",
                            colClasses = c(
                              # DynamicInfoAvailable = "character",
                              IsOpen24Hours = "logical",
                              LastUpdate = "character",
                              MaxCapacity = "numeric",
                              ChargingFacility_power = "numeric"
                              ),
                            stringsAsFactors = TRUE
)
  select(EvseID,ChargingStationId,Accessibility,GeoCoordinates,City,Country,PostalCode,Street,IsOpen24Hours,LastUpdate,DynamicInfoAvailable,DynamicPowerLevel,ChargingFacility_power,ChargingFacility_powertype) %>%
  separate(GeoCoordinates, into = c("latitude", "longitude"), sep = " ", convert = TRUE)

CH_EVSE_details$LastUpdate <- ymd_hms(CH_EVSE_details$LastUpdate, tz = "UTC")

head(CH_EVSE_details)
```


```{r}
summary(CH_EVSE_details)

```

```{r Cleaning the charger dataset}
# CH_EVSE_details[CH_EVSE_details$longitude < 6, ]

CH_EVSE_details_cleaned <- CH_EVSE_details

#convert longitude and latitude to NA if they fall outside of the country extents
CH_EVSE_details_cleaned$longitude <- ifelse(
  CH_EVSE_details_cleaned$longitude >= 5.5 & CH_EVSE_details_cleaned$longitude <= 10.5,
  CH_EVSE_details_cleaned$longitude,
  NA)

CH_EVSE_details_cleaned$latitude <- ifelse(
  CH_EVSE_details_cleaned$latitude >= 45.5 & CH_EVSE_details_cleaned$latitude <= 47.9,
  CH_EVSE_details_cleaned$latitude,
  NA)

CH_EVSE_details_cleaned
```

```{r}
summary(CH_EVSE_details_cleaned)
write.csv(CH_EVSE_details_cleaned, "M:/Rudolph/R-Projects/swiss-public-charging/charging_stations_cleaned_2025-07.csv")

```


```{r summarise charging point dataset}
hist(CH_EVSE_details_cleaned$ChargingFacility_power, xlab = "Charge Point Power [kW]", main = "")
# boxplot(CH_EVSE_details_cleaned$ChargingFacility_power)
```


```{r function to convert charging availability tables - monthly data}

EVSE_processing <- function(csv_file, merged_object = NULL, id_col = "EvseID") { #file path as a string input
  # Read the csv file into a dataframe object
  my.df <- read.csv(csv_file)
  
  # Convert the availability statuses into factors (to save storage space)
  factor_cols <- 2:dim(my.df)[2]
  my.df[ , factor_cols] <- lapply(my.df[ , factor_cols],factor)

  # (Optional) Merge with an existing object
  if(!is.null(merged_object)) {
    result.df <- merge(merged_object, my.df, by = id_col, all = TRUE)
    return(result.df) #return a merged dataframe if one is defined
  }
  return(my.df) #return the single processed data frame
}

```

```{r function to cycle through files in a folder - monthly data}
read_csvs <- function(main_dir = monthly_chg_files, start_f = 1, n_files = 'All') {
  # Get list of sub-directories in the main directory
  # subdirs <- list.dirs(main_dir, full.names = TRUE, recursive = FALSE)
  
  # Get list of files in the main directory
  if(n_files == 'All')  file_list <- list.files(main_dir)
  # to do a subset of the files
  else {
    end_f = start_f + n_files - 1
    file_list <- list.files(main_dir)[start_f:end_f]
  }
  
  df_statuses <- NULL #initiate NULL for the dataframe
  
  # Loop through each sub directory
  for (file in file_list) {
    start_time <- Sys.time()

    # Create the full path to the CSV file
    csv_file_path <- file.path(main_dir, file)
    
    if(!file.exists(csv_file_path)) next #if the file does not exist, then go to the next one.
    
    if(is.null(df_statuses)) { #the first time this is run, df_statuses will be NULL
      # Read and process the csv
      df_statuses <- EVSE_processing(csv_file_path)
    } else {
      # Read, process, and merge the csv with existing ones
      df_statuses <- EVSE_processing(csv_file_path, merged_object = df_statuses)
    }
    end_time <- Sys.time()

    execution_time <- end_time - start_time

    cat("Execution time was", as.numeric(execution_time, units = "secs"), "s for this iteration.\n")

  }
  
  return(df_statuses)
}

```

```{r Spatial join of charging infrastructure}

library(sf)

#load Swiss Cantonal boundaries shapefile
CH_Cantons <- st_read("M:/Rudolph/0-Datasets/Switzerland/Swiss_admin_boundaries_shapefiles/swissBOUNDARIES3D_1_5_TLM_KANTONSGEBIET.shp")

#load Swiss District boundaries shapefile
CH_Districts <- st_read("M:/Rudolph/0-Datasets/Switzerland/Swiss_admin_boundaries_shapefiles/swissBOUNDARIES3D_1_5_TLM_BEZIRKSGEBIET.shp")

CH_Municipality <- st_read("M:/Rudolph/0-Datasets/Switzerland/Swiss_admin_boundaries_shapefiles/swissBOUNDARIES3D_1_5_TLM_HOHEITSGEBIET.shp")

#convert charging data "CH_EVSE_details_short" into an sf format
CH_EVSE_sf <- st_as_sf(CH_EVSE_details_short, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(., st_crs(CH_Cantons))

#spatial join of cantons
CH_EVSE_locations <- st_join(CH_EVSE_sf, CH_Cantons %>% select(NAME), join = st_within) %>%
  rename(Canton = NAME)

#spatial join of districts
CH_EVSE_locations <- st_join(CH_EVSE_locations, CH_Districts %>% select(NAME), join = st_within) %>%
  rename(District = NAME)

CH_EVSE_df <- CH_EVSE_locations %>%
  st_drop_geometry()

```

```{r}
Muni_list <- unique(CH_Municipality$NAME)

CH_Territory[CH_Territory$NAME == "Baden", ]

length(unique(CH_Districts$NAME))
```

```{r Total Swiss Charging Aggregated}
CH_chgpts <- read.csv("M:/Rudolph/0-Datasets/Switzerland/Swiss_Charging_Stock_April2025.csv") 

CH_chgpts_pwr <- CH_chgpts %>%
  select(year,month,contains("kW_count")) %>%
  rename(count_10 = chargingPower_10kW_count,
         count_21 = chargingPower_21kW_count,
         count_42 = chargingPower_42kW_count,
         count_100 = chargingPower_100kW_count,
         count_100plus = chargingPower_100pluskW_count)

colnames(CH_chgpts_pwr[,-c(year,month)]) <- gsub("^[^_]*_([^_]+)_.*$", "\\1", colnames(CH_chgpts_pwr[,-c(year,month)]))



```


```{r function to read in csvs}
csv_to_df <- function(file) {
  result <- read.csv(file, as.is = "EvseID", stringsAsFactors = TRUE)
  
  # Find the position of "EvseID" column
  evse_col <- which(names(result) == "EvseID")

  # Keep columns starting from "EvseID"
  result <- result[, evse_col:ncol(result)]
  
  return(result)
}

```

```{r to read the "EVSE_partial" datasets into dfs - IF NEEDED}
EVSE_partial.1 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.1.csv")
EVSE_partial.2 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.2.csv")
EVSE_partial.3 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.3.csv")
EVSE_partial.4 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.4.csv")
EVSE_partial.5 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.5.csv")
EVSE_partial.6 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.6.csv")

```



#### Adding columns to processed charging dataset
```{r function to merge power level of each charger}
merge_power_location <- function(EVSE_partial_df) {
  EVSE_df <- merge(EVSE_partial_df, CH_EVSE_details_cleaned[ , c('EvseID','ChargingFacility_power','City')], by = 'EvseID', all.x = TRUE)

  #If any unmatched rows, attempt to match EvseID with ChargingStationId instead
  unmatched <- is.na(EVSE_df$ChargingFacility_power)
  
  if(any(unmatched)) {
    print("unmatched rows exist")
    unmatched_df <- EVSE_partial_df[unmatched, ]
  # Merge unmatched rows on ChargingStationId instead
  
    fallback_merge <- merge(unmatched_df, CH_EVSE_details_cleaned[ , c('ChargingStationId','ChargingFacility_power','City')],
      by.x = 'EvseID',
      by.y = 'ChargingStationId',
      all.x = TRUE
    )
   
    EVSE_df <- EVSE_df[!unmatched, ]
    EVSE_df <- rbind(EVSE_df, fallback_merge)

    print("merged df created")
  }
    
  EVSE_df <- EVSE_df %>%
    relocate(City, .after = EvseID) %>%
    relocate(ChargingFacility_power, .after = EvseID)
  
  return(EVSE_df)
}
```


```{r add charge point data (power and location) to the partial dfs}
EVSE_df.1 <- merge_power_location(EVSE_partial.1)
summary(EVSE_df.1$ChargingFacility_power)
EVSE_df.2 <- merge_power_location(EVSE_partial.2)
EVSE_df.3 <- merge_power_location(EVSE_partial.3)
EVSE_df.4 <- merge_power_location(EVSE_partial.4)
EVSE_df.5 <- merge_power_location(EVSE_partial.5)
EVSE_df.6 <- merge_power_location(EVSE_partial.6)
summary(EVSE_df.6$ChargingFacility_power)
```

```{r Check how many EvseIDs are not matching (we don't have data for them)}
length(EVSE_df.1$EvseID) #14350
length(EVSE_df.6$EvseID) #23485
length(CH_EVSE_details$EvseID) #19307
matches <- CH_EVSE_details$EvseID %in% EVSE_df.6$EvseID 
no_matches <- !matches
no_matches_EvseID <- CH_EVSE_details$EvseID[no_matches]

num_nomatches <- sum(no_matches)

EVSE_df.6_nomatch <- EVSE_df.6[no_matches,]

CH_EVSE_details_nomatch <- CH_EVSE_details[CH_EVSE_details$EvseID %in% no_matches_EvseID, ]

station_matches <- CH_EVSE_details_nomatch$ChargingStationId %in% no_matches_EvseID

sum(station_matches)

no_matches_stations <- CH_EVSE_details_nomatch[station_matches, ]


```

```{r Data snipit}
Full_EVSE_df <- EVSE_df.6 %>%
  select(EvseID,Power_kW, City)

unique(Full_EVSE_df$City)

write.csv(Full_EVSE_df, "M:/Rudolph/0-Datasets/Switzerland/Charging_point_IDs.csv")
```



```{r function to get the utilization rate of each collected timestamp}
availability_summary <- function(df_input) { #takes the df object from function "read_csvs()" as the input
  # df_util <- data.frame(timestamp = NA, year = NA, month = NA, day = NA, weekday = NA, hour = NA, minute = NA, occupied = NA, available = NA, unknown = NA, service = NA, NA_ = NA, total = NA, utilization = NA)
  first <- TRUE
  for(col in colnames(df_input)) {
    if (!grepl("^S_", col)) next
    num_used <- sum(df_input[ ,col] == "Occupied", na.rm = TRUE) #number of occupied CPs
    num_available <- sum(df_input[ ,col] == "Available", na.rm = TRUE) #number of available CPs
    num_unknown <- sum(df_input[ ,col] == "Unknown", na.rm = TRUE) #number of unknown CPs
    num_service <- sum(df_input[ ,col] == "OutOfService", na.rm = TRUE) #number of out of service CPs
    num_NA <- sum(is.na(df_input[ ,col]), na.rm = TRUE) 
    tot_num <- length(df_input[ ,col])
    utilization_rate <- num_used/(num_used + num_available)
    
    #convert column name to a datetime object
    datetime_string <- sub("S_","",col)
    datetime_object <- ymd_hms(gsub("\\.", ":", datetime_string), tz = "Europe/Zurich")
    
    #record the data in a new results dataframe
    row_util <- data.frame(timestamp = datetime_object,
                           year = year(datetime_object), 
                           month = month(datetime_object),
                           day = day(datetime_object),
                           weekday = wday(datetime_object),
                           hour = hour(datetime_object),
                           minute = round(minute(datetime_object)/5)*5,
                           occupied = num_used, 
                           available = num_available, 
                           unknown = num_unknown, 
                           service = num_service,
                           NA_ = num_NA,
                           total = tot_num, 
                           utilization = utilization_rate)
    
    #make sure specific columns are integers
    int_cols <- c("year","month","day","weekday","hour","minute","occupied","available","unknown","service","NA_","total")
    row_util[ ,int_cols] <- lapply(row_util[ ,int_cols], as.integer)
    
    #bind with the large df
    if(first == TRUE) { #if this is the first row of the df
      df_util <- row_util
      first <- FALSE 
    } else {
      df_util <- rbind(df_util, row_util)
    }
    
  }
  return(df_util)
}

```


```{r SANDBOX}
EVSE_util.1 <- availability_summary(EVSE_df.1[ , 1:1000])

EVSE_util.1.winter <- EVSE_util.1 %>%
  filter(month %in% c(12,1,2))

summary(EVSE_util.1$month)
```
```{r segment by power level - NOT USED YET}
# summary(EVSE_df$Power_kW)
EVSE_lowP <- EVSE_df %>%
  filter(Power_kW <= 22)

EVSE_medP <- EVSE_df %>%
  filter(Power_kW <= 100 & Power_kW > 22)

EVSE_highP <- EVSE_df %>% 
  filter(Power_kW > 100)

EVSE_unknownP <- EVSE_df %>%
  filter(is.na(Power_kW))
```

```{r Utilization rate values - by power level - NOT USED YET}
#takes some time to run

EVSE_util_lowP <- Util_rate(EVSE_lowP)

# EVSE_util_medP <- Util_rate(EVSE_medP)

# EVSE_util_highP <- Util_rate(EVSE_highP)

EVSE_util_ovr <- Util_rate(EVSE_df)


```


### Utilization rate curves of each day type
```{r get utilization rate for all times in each day type}
# rm(list = c("win_wk_util","win_sa_util","win_su_util",
#             "spr_wk_util","spr_sa_util","spr_su_util",
#             "sum_wk_util","sum_sa_util","sum_su_util",
#             "fal_wk_util","fal_sa_util","fal_su_util"))

df_list <- c("EVSE_df.1","EVSE_df.2","EVSE_df.3","EVSE_df.4","EVSE_df.5","EVSE_df.6") #include more if there are other datasets included now

#NOTE: If you want to subset by something else (power level, location, etc) that must be done before this step.

for(df in 1:length(df_list)) {
  this_df <- get(df_list[df])
  this_df <- availability_summary(this_df) #get the utilization and time details from this partial dataset
  this_df$hour[this_df$hour == 0] <- 24 #as per conventions, hour 0 should be converted to hour 24
  #check
  leftover_rows <- nrow(this_df)
  
  #Note: Seasonal boundary months defined the same as in STEM-E
  #winter
  win_df <- this_df %>%
    filter(month %in% c(11,12,1))
  
  if(!exists("win_wk_util")) {
    win_wk_util <- filter(win_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(win_wk_util)
  } else {
    this_util <- filter(win_df, weekday %in% c(1,2,3,4,5))
    win_wk_util <- rbind(win_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("win_sa_util")) {
    win_sa_util <- filter(win_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(win_sa_util)
  } else {
    this_util <- filter(win_df, weekday == 6)
    win_sa_util <- rbind(win_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("win_su_util")) {
    win_su_util <- filter(win_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(win_su_util)
  } else {
    this_util <- filter(win_df, weekday == 7)
    win_su_util <- rbind(win_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  #spring
  spr_df <- this_df %>%
    filter(month %in% c(2,3,4))

  if(!exists("spr_wk_util")) {
    spr_wk_util <- filter(spr_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(spr_wk_util)
  } else {
    this_util <- filter(spr_df, weekday %in% c(1,2,3,4,5))
    spr_wk_util <- rbind(spr_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("spr_sa_util")) {
    spr_sa_util <- filter(spr_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(spr_sa_util)
  } else {
    this_util <- filter(spr_df, weekday == 6)
    spr_sa_util <- rbind(spr_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("spr_su_util")) {
    spr_su_util <- filter(spr_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(spr_su_util)
  } else {
    this_util <- filter(spr_df, weekday == 7)
    spr_su_util <- rbind(spr_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  #summer
  sum_df <- this_df %>%
    filter(month %in% c(5,6,7))
  
  if(!exists("sum_wk_util")) {
    sum_wk_util <- filter(sum_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(sum_wk_util)
  } else {
    this_util <- filter(sum_df, weekday %in% c(1,2,3,4,5))
    sum_wk_util <- rbind(sum_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("sum_sa_util")) {
    sum_sa_util <- filter(sum_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(sum_sa_util)
  } else {
    this_util <- filter(sum_df, weekday == 6)
    sum_sa_util <- rbind(sum_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("sum_su_util")) {
    sum_su_util <- filter(sum_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(sum_su_util)
  } else {
    this_util <- filter(sum_df, weekday == 7)
    sum_su_util <- rbind(sum_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  #fall
  fal_df <- this_df %>%
    filter(month %in% c(8,9,10))
  
  if(!exists("fal_wk_util")) {
    fal_wk_util <- filter(fal_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(fal_wk_util)
  } else {
    this_util <- filter(fal_df, weekday %in% c(1,2,3,4,5))
    fal_wk_util <- rbind(fal_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("fal_sa_util")) {
    fal_sa_util <- filter(fal_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(fal_sa_util)
  } else {
    this_util <- filter(fal_df, weekday == 6)
    fal_sa_util <- rbind(fal_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("fal_su_util")) {
    fal_su_util <- filter(fal_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(fal_su_util)
  } else {
    this_util <- filter(fal_df, weekday == 7)
    fal_su_util <- rbind(fal_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  print(leftover_rows)
}

rm(list = c("spr_df","sum_df","fal_df","win_df")) #these were temporarily assigned in each loop


```


```{r}
get_timeslice_ci <- function(util_df, hrs = seq(1,24), z = 1.96) {
  max <- c()
  high <- c()
  median <- c()
  mean <- c()
  low <- c()
  min <- c()
  
  if(min(util_df$utilization, na.rm = TRUE) <= 0) warning("There are negative utilization rates in the dataframe.")
  if(sum(is.na(util_df$utilization)) > 0 ) warning("There are NA values for utilization rates in the dataframe.")
  
  for(hr in hrs) {
    filtered_df <- util_df %>%
      filter(hour == hr) 
  
    utilization <- filtered_df$utilization
    
    sample.mean <- mean(utilization, na.rm = TRUE)
    sample.median <- median(utilization, na.rm = TRUE)
    sample.sd <- sd(utilization, na.rm = TRUE)
    sample.n <- length(utilization)
    sample.min <- min(utilization, na.rm = TRUE)
    sample.max <- max(utilization, na.rm = TRUE)
    
    CI.high <- sample.mean + z*(sample.sd/sqrt(sample.n))
    CI.low <- sample.mean - z*(sample.sd/sqrt(sample.n))
    
    #collect values into vectors
    max <- c(max,sample.max)
    high <- c(high,CI.high)
    median <- c(median,sample.median)
    mean <- c(mean,sample.mean)
    low <- c(low,CI.low)
    min <- c(min,sample.min)
    
  }
  
  #return the dataframe of the vectors collected
  return(data.frame("hour" = hrs,
                    "max" = max,
                    "CI_high" = high,
                    "median" = median,
                    "mean" = mean,
                    "CI_low" = low,
                    "min" = min))
  
}
```


```{r Get overall utilization curves}
daytype_names <- c("win_wk","win_sa","win_su",
                   "spr_wk","spr_sa","spr_su",
                   "sum_wk","sum_sa","sum_su",
                   "fal_wk","fal_sa","fal_su")

# day <- daytype_names[2]

for(day in daytype_names) {
  CI_df_name <- paste0(day,"_CI")
  df_name <- paste0(day,"_util")
  
  this_df <- get(df_name)
  
  this_CI_df <- get_timeslice_ci(this_df, z = 2.576)
  
  assign(CI_df_name, this_CI_df)
}

#Result of this block is to have several day type dataframes with the confidence interval, minimum, and maximums over each hour (and therefore, each timeslice)
```

```{r Save full utilization dataframes}
daytype_folder <- "M:/Rudolph/R-Projects/swiss-public-charging/data/daytype_dataframes"
for(day in daytype_names) {
  df_name <- paste0(day,"_util")
  csv_name <- paste0(df_name,".csv")
  
  this_df <- get(df_name)
  
  filepath <- file.path(daytype_folder, csv_name)  
  write.csv(this_df, filepath, row.names = FALSE)
  
}
```


```{r Save confidence interval dataframes}
daytype_ci_folder <- "M:/Rudolph/R-Projects/swiss-public-charging/data/daytype_ci"
for(day in daytype_names) {
  CI_df_name <- paste0(day,"_CI")
  csv_name <- paste0(CI_df_name,".csv")
  
  this_df <- get(CI_df_name)
  
  filepath <- file.path(daytype_ci_folder, csv_name)  
  write.csv(this_df, filepath, row.names = FALSE)
}
  
```

```{r Create and save a df that can be copy/pasted into VEDA}
#needs a column for timeslice in the form SPR-WK-D01 and the utilization rate value (as a decimal)
hrs <- seq(1,24)
TimeSlice <- c()
Utilization_min <- c()
Utilization_median <- c()
Utilization_max <- c()

day <- daytype_names[1]
for(day in daytype_names) {
  this_df_name <- paste0(day,"_CI")
  this_df <- get(this_df_name)
  for(hr in hrs) {
    this_TS_name <- sprintf("%s-D%02d", gsub("_", "-", toupper(day)), hr) #correctly formats the string to fit VEDA needs
    this_utilization_min <- this_df[this_df$hour == hr, "min"]
    this_utilization_median <- this_df[this_df$hour == hr, "median"]
    this_utilization_max <- this_df[this_df$hour == hr, "max"]
    
    TimeSlice <- c(TimeSlice,this_TS_name)
    Utilization_min <- c(Utilization_min,this_utilization_min)
    Utilization_median <- c(Utilization_median,this_utilization_median)
    Utilization_max <- c(Utilization_max,this_utilization_max)

  }
}

result <- data.frame("TimeSlice" = TimeSlice,
                     "Utilization_min" = Utilization_min,
                     "Utilization_median" = Utilization_median,
                     "Utilization_max" = Utilization_max)



```

```{r}
write.csv(result,  "M:/Rudolph/R-Projects/swiss-public-charging/data/timeslice_ci.csv")
```


#Visualizations of utlization
```{r Visualization function}
visualize_utilization_summary <- function(util_CI_df, title = "Utilization Summary") {
  y_max <- ceiling(max(util_CI_df$max)*100)
  y_max <- y_max + 5 - (y_max %% 5) #gets the maximum value up to the nearest 5
  
  plot(x = util_CI_df$hour, y = util_CI_df$median*100, 
     ylim = c(0,y_max), ylab = "Utilization [%]", xlab = "Hour", 
     main = title,
     col = 'black', cex = 0.8, type = 'l')
  lines(x = util_CI_df$hour, y = util_CI_df$CI_low*100, col = 'blue')
  lines(x = util_CI_df$hour, y = util_CI_df$CI_high*100, col = 'blue')
  lines(x = util_CI_df$hour, y = util_CI_df$min*100, col = 'red')
  lines(x = util_CI_df$hour, y = util_CI_df$max*100, col = 'red')

}

```

```{r}
visualize_utilization_summary(win_wk_CI)
```

NOTE: 
this confidence interval is super tight which indicates that there could be some autocorrelation between the hours, plus the fact that I have 20 observations per hour (every 5 minutes. I might want to consider a bootstrap method to get the confidence interval in a future iteration.)
```{r Bootstrap code}

bootstrap_ci <- function(data, num_bootstrap_samples = 10000, ci = 95) {
  boot_samples <- replicate(num_bootstrap_samples, median(sample(data, replace = TRUE)))
  
  lower <- quantile(boot_samples, (100 - ci) / 2 / 100)
  upper <- quantile(boot_samples, 1 - (100 - ci) / 2 / 100)
  
  return(c(lower, upper))
}

# Compute bootstrap confidence intervals for each hour
hourly_cis <- df %>%
  group_by(hour) %>%
  summarise(
    Lower_CI = bootstrap_ci(utilization_rate)[1],
    Upper_CI = bootstrap_ci(utilization_rate)[2]
  )

# Print the results
print(hourly_cis)

```







```{r function to get certain parameters}
Subset_EVSE <- function(df_input, year.f = NULL, month.f = NULL, day.f = NULL, weekday.f = NULL, hour.f = NULL) {
  df_result <- df_input
  
  if(!is.null(year.f)) {df_result <- filter(df_result, year == year.f)}
  if(!is.null(month.f)) {df_result <- filter(df_result, month == month.f)}
  if(!is.null(day.f)) {df_result <- filter(df_result, day == day.f)}
  if(!is.null(weekday.f)) {df_result <- filter(df_result, weekday == weekday.f)}
  if(!is.null(hour.f)) {df_result <- filter(df_result, hour == hour.f)}
  
  return(df_result)
}

Get_CI <- function(df_input, col_name = "utilization", z_score = 1.96, clean.na = TRUE) {
  observations <- df_input[col_name]
  
  if(clean.na == TRUE)  observations <- observations[!is.na(observations)]
  
  sample_mean <- mean(observations)
  sample_sd <- sd(observations)
  sample_size <- length(observations)
  sample_min <- min(observations)
  sample_max <- max(observations)
  
  sample_median <- median(observations)
  
  std_err <- sample_sd/ sqrt(sample_size)
  
  lower_bound <- sample_mean - (z_score * std_err)
  upper_bound <- sample_mean + (z_score * std_err)
  
  
  return(list("lower_bound" = lower_bound,
              "mean" = sample_mean,
              "upper_bound" = upper_bound,
              "min" = sample_min,
              "max" = sample_max))
}
```

```{r sandbox}
filter(EVSE_util_wkday, !is.na("utilization"))

undebug(Get_CI)
Weekday.CI <- Get_CI(EVSE_util_wkday)
```


```{r weedays}
EVSE_util_wkday <- EVSE_util_lowP %>%
  filter(weekday <= 5)


#get the confidence interval for each hour
for(h in 0:23) {
  this.df <- Subset_EVSE(EVSE_util_wkday, hour.f = h)
  
  this.CI <- Get_CI(this.df)
  
  result.df <- data.frame(hour = h, 
                          lower_bound = this.CI$lower_bound,
                          mean = this.CI$mean,
                          upper_bound = this.CI$upper_bound,
                          min = this.CI$min,
                          max = this.CI$max)
  
  if(h == 0) {
    result <- result.df
  } else {
    result <- rbind(result, result.df)
    }
}

#visualizations
result

plot(x = EVSE_util_wkday$hour, y = EVSE_util_wkday$utilization*100, 
     ylim = c(0,20), ylab = "Utilization [%]", xlab = "Hour", 
     col = 'black', pch = 19, cex = 0.8)

plot(result$hour, result$mean*100, type = 'l', col = 'black', 
     xlab = "hour", ylab = "utilization [%]", ylim = c(0,20),
     main = "Weekdays")
lines(result$hour, result$lower_bound*100, type = 'l', col = 'blue')
lines(result$hour, result$upper_bound*100, type = 'l', col = 'blue')
lines(result$hour, result$min*100, type = 'l', col = 'red')
lines(result$hour, result$max*100, type = 'l', col = 'red')

```
```{r}
hist(filter(result, hour = 10))
```

```{r clean data}
EVSE_util_df_cleaned <- EVSE_util_df %>%
  filter(unknown < dim(EVSE_util_df)[1] * 0.1) %>% #remove instances where 'unknown' CPs are too high, this is likely a data collection error
  merge(Charger_df, Status_df, by = "EvseID", all = TRUE)

```

```{r plot utilization}
plot(x = as.POSIXlt(EVSE_util_df_cleaned$timestamp), y = EVSE_util_df_cleaned$utilization, pch = 19,
     ylab = "Utilization Rate", xlab = "Date", ylim = c(0,0.25))

```


```{r filter for observations}

status_filter <- function(data, )

```

```{r EVSE details}

Power_demand <- function(Charger_df, Status_df) {
  Charger_df <- Charger_df %>%
    select(EvseID, Power_kW)
  
  # Add the power demand column to the status data frame
  Power_status_df <- merge(Charger_df, Status_df, by = "EvseID", all = TRUE)
  
  # convert "occupied" into the power level for all the status columns
 for(col in 3:dim(Power_status_df)[2]) {
   Power_status_df[ , col] <- Power_status_df[ , "Power_kW"] * as.numeric(Power_status_df[ , col] == "Occupied")
 }
  return(Power_status_df) 
}

```

```{r convert statuses into power level demands}
EVSE_df <- Power_demand(CH_EVSE_details_short, EVSE_partial.B)
```


```{r clean power status}
Power_status_df_cleaned <- Power_status_df %>%
  filter(!is.na(Power_kW))
```


```{r power level}
Power_level <- function(df_input, filters = NULL) { #takes the df object from function "Power_demand()" as the input
  df_power <- data.frame(timestamp = NA, year = NA, month = NA, day = NA, weekday = NA, hour = NA, minute = NA, power = NA)
  for(col in colnames(df_input[-c(1:2)])) {
    power_dem <- sum(df_input[ , col], na.rm = TRUE) #gets the total power level
    
    #convert column name to a datetime object
    datetime_string <- sub("S_","",col)
    datetime_object <- ymd_hms(gsub("\\.", ":", datetime_string), tz = "Europe/Zurich")
    
    #record the data in a new results dataframe
    row_pwr <- data.frame(timestamp = datetime_object,
                           year = year(datetime_object), 
                           month = month(datetime_object),
                           day = day(datetime_object),
                           weekday = wday(datetime_object),
                           hour = hour(datetime_object),
                           minute = round(minute(datetime_object)/5)*5,
                           power = power_dem)
    
    #bind with the large df
    df_power <- rbind(df_power, row_pwr)
  }
  return(df_power)
}

```

```{r power level data}
#takes time to run
Power_results <- Power_level(Power_status_df_cleaned)
```
```{r power level data cleaned}
Power_results_cleaned <- Power_results %>%
  filter(power > 0)

```


```{r plot power}
plot(x = as.POSIXlt(Power_results_cleaned$timestamp), y = Power_results_cleaned$power/1000, pch = 19,
     ylab = "Power Demand [MW]", xlab = "Date", ylim = c(0,20))
```

