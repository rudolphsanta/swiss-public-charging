---
title: "Swiss Public Charging Data Analysis"
output: html_document
date: "2024-11-04"
---

```{r setup, include=FALSE}
library(pxR)
library(dplyr)
library(tidyverse)
library(csv)
library(readr)
library(data.table)
library(arrow)
library(data.table)  # optional for rbindlist if you prefer
library(lubridate)
library(parallel)

# charging status file location and list
charging_data_root <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_Parquet")

chg_avail_files <- list.files(charging_data_root,full.names = TRUE)

```


# Charging point descriptions
```{r Charging Point Details}
CH_EVSE_details <- read.csv("M:/Rudolph/R-Projects/swiss-public-charging/data/ChargingStationDetails_10-10-2025.csv",
                            colClasses = c(
                              # DynamicInfoAvailable = "character",
                              IsOpen24Hours = "logical",
                              LastUpdate = "character",
                              MaxCapacity = "numeric",
                              ChargingFacility_power = "numeric"
                              ),
                            stringsAsFactors = TRUE
) %>%
  # select(EvseID,ChargingStationId,Accessibility,GeoCoordinates,City,Country,PostalCode,Street,IsOpen24Hours,LastUpdate,DynamicInfoAvailable,DynamicPowerLevel,ChargingFacility_power,ChargingFacility_powertype) %>%
  separate(GeoCoordinates, into = c("latitude", "longitude"), sep = " ", convert = TRUE)

summary(CH_EVSE_details)
dim(CH_EVSE_details)

CH_EVSE_details$LastUpdate <- ymd_hms(CH_EVSE_details$LastUpdate, tz = "UTC")

#convert longitude and latitude to NA if they fall outside of the country extents
CH_EVSE_details$longitude <- ifelse(
  CH_EVSE_details$longitude >= 5.5 & CH_EVSE_details$longitude <= 10.5,
  CH_EVSE_details$longitude,
  NA)

CH_EVSE_details$latitude <- ifelse(
  CH_EVSE_details$latitude >= 45.5 & CH_EVSE_details$latitude <= 47.9,
  CH_EVSE_details$latitude,
  NA)

#remove any rows that are EXACT duplicates (duplicated across all columns)
CH_EVSE_details <- CH_EVSE_details[!duplicated(CH_EVSE_details), ]

summary(CH_EVSE_details)
dim(CH_EVSE_details)

#write.csv(CH_EVSE_details, "M:/Rudolph/R-Projects/swiss-public-charging/charging_stations_cleaned_2025-07.csv")

```



```{r correlation heatmap}
# Load required packages
library(lsr)    # for etaSquared
library(vcd)    # for assocstats (Cramer's V)
library(dplyr)

# Subset relevant variables
df <- CH_EVSE_details %>%
  select(Accessibility, ChargingFacility_powertype,
         latitude, longitude, ChargingFacility_power)

# Identify variable types
cat_vars <- c("Accessibility", "ChargingFacility_powertype")
cont_vars <- c("latitude", "longitude", "ChargingFacility_power")

# Initialize correlation matrix
vars <- c(cat_vars, cont_vars)
corr_mat <- matrix(NA, nrow = length(vars), ncol = length(vars),
                   dimnames = list(vars, vars))

# Compute correlations
for (i in seq_along(vars)) {
  for (j in seq_along(vars)) {
    v1 <- df[[vars[i]]]
    v2 <- df[[vars[j]]]
    
    # Continuous–continuous: Pearson
    if (vars[i] %in% cont_vars & vars[j] %in% cont_vars) {
      corr_mat[i, j] <- cor(v1, v2, use = "pairwise.complete.obs", method = "pearson")
      
    # Categorical–categorical: Cramer's V
    } else if (vars[i] %in% cat_vars & vars[j] %in% cat_vars) {
      tbl <- table(v1, v2)
      corr_mat[i, j] <- suppressWarnings(assocstats(tbl)$cramer)
      
    # Categorical–continuous: eta (correlation ratio)
    } else if (vars[i] %in% cat_vars & vars[j] %in% cont_vars) {
      corr_mat[i, j] <- sqrt(etaSquared(aov(v2 ~ v1), anova = TRUE)[1, "eta.sq"])
    } else if (vars[i] %in% cont_vars & vars[j] %in% cat_vars) {
      corr_mat[i, j] <- sqrt(etaSquared(aov(v1 ~ v2), anova = TRUE)[1, "eta.sq"])
    }
  }
}

# Convert to data frame for easy viewing
corr_df <- as.data.frame(corr_mat)
# print(round(corr_df, 2))
corr_mat_num <- apply(corr_mat, 2, as.numeric)
rownames(corr_mat_num) <- rownames(corr_mat)

#Plot
par(mar = c(1, 1, 3, 1))   # smaller outer margins (bottom, left, top, right)
par(oma = c(0, 0, 0, 0))   # outer margins to zero
par(cex = 0.7)             # scale all text elements slightly larger
corrplot(corr_mat_num,
         method = "color",        # colored boxes
         type = "upper",          # show only upper triangle
         tl.col = "black",        # text color for variable names
         tl.srt = 45,             # rotate variable labels
         addCoef.col = "black",   # show correlation values
         number.cex = 1.2,        # size of correlation numbers
         tl.cex = 1.1,            # size of variable labels
         mar = c(1, 1, 3, 1),     # margin size
         title = "Mixed Correlation Matrix: CH_EVSE_data",
         cl.cex = 1.0,            # color legend size
         col = colorRampPalette(c("darkblue", "white", "darkred"))(200),
         na.label = " ")     


```


```{r}
CH_EVSE_details2 <- CH_EVSE_details %>%
  mutate(ChargingFacility_powertype_cat = case_when(
    is.null(ChargingFacility_powertype) ~ 0,
    ChargingFacility_powertype == "AC_1_PHASE" ~ 1,
    ChargingFacility_powertype == "AC_3_PHASE" ~ 2,
    ChargingFacility_powertype == "DC" ~ 3,
    TRUE ~ NA_real_   # fallback for unexpected values
  ))
```


```{r}
plot(x = CH_EVSE_details2$ChargingFacility_power, y = CH_EVSE_details2$ChargingFacility_powertype_cat)
```

```{r}
CH_EVSE_details_empty <- filter(CH_EVSE_details2, ChargingFacility_power == 0 | is.na(ChargingFacility_power))
CH_EVSE_details_AC1 <- filter(CH_EVSE_details, ChargingFacility_powertype == "AC_1_PHASE")
summary(CH_EVSE_details_AC1)
```
```{r summarize missing}
summary_missing <- sapply(CH_EVSE_details, function(x) {
  # Start with NAs
  missing_count <- sum(is.na(x))
  
  # Add zeros (only for numeric columns)
  if (is.numeric(x)) {
    missing_count <- missing_count + sum(x == 0, na.rm = TRUE)
  }
  
  # Add empty strings (only for character or factor columns)
  if (is.character(x) || is.factor(x)) {
    missing_count <- missing_count + sum(trimws(as.character(x)) == "", na.rm = TRUE)
  }
  
  return(missing_count)
})

summary_missing <- sort(summary_missing, decreasing = TRUE)
print(summary_missing)
```


```{r summarise charging point dataset}
hist(CH_EVSE_details$ChargingFacility_power, xlab = "Charge Point Power [kW]", main = "")
# boxplot(CH_EVSE_details_cleaned$ChargingFacility_power)
```

```{r Spatial join of charging infrastructure}

library(sf)

#load Swiss Cantonal boundaries shapefile
CH_Cantons <- st_read("M:/Rudolph/0-Datasets/Switzerland/Swiss_admin_boundaries_shapefiles/swissBOUNDARIES3D_1_5_TLM_KANTONSGEBIET.shp")

#load Swiss District boundaries shapefile
CH_Districts <- st_read("M:/Rudolph/0-Datasets/Switzerland/Swiss_admin_boundaries_shapefiles/swissBOUNDARIES3D_1_5_TLM_BEZIRKSGEBIET.shp")

CH_Municipality <- st_read("M:/Rudolph/0-Datasets/Switzerland/Swiss_admin_boundaries_shapefiles/swissBOUNDARIES3D_1_5_TLM_HOHEITSGEBIET.shp")

#convert charging data "CH_EVSE_details_short" into an sf format
CH_EVSE_sf <- st_as_sf(CH_EVSE_details_short, coords = c("longitude", "latitude"), crs = 4326) %>%
  st_transform(., st_crs(CH_Cantons))

#spatial join of cantons
CH_EVSE_locations <- st_join(CH_EVSE_sf, CH_Cantons %>% select(NAME), join = st_within) %>%
  rename(Canton = NAME)

#spatial join of districts
CH_EVSE_locations <- st_join(CH_EVSE_locations, CH_Districts %>% select(NAME), join = st_within) %>%
  rename(District = NAME)

CH_EVSE_df <- CH_EVSE_locations %>%
  st_drop_geometry()

```

```{r Total Swiss Charging Aggregated}
CH_chgpts <- read.csv("M:/Rudolph/0-Datasets/Switzerland/Swiss_Charging_Stock_April2025.csv") 

CH_chgpts_pwr <- CH_chgpts %>%
  select(year,month,contains("kW_count")) %>%
  rename(count_10 = chargingPower_10kW_count,
         count_21 = chargingPower_21kW_count,
         count_42 = chargingPower_42kW_count,
         count_100 = chargingPower_100kW_count,
         count_100plus = chargingPower_100pluskW_count)

colnames(CH_chgpts_pwr[,-c(year,month)]) <- gsub("^[^_]*_([^_]+)_.*$", "\\1", colnames(CH_chgpts_pwr[,-c(year,month)]))



```

```{r filter EVSE}
EVSE_filter <- function(EVSE_df, Latitude = NULL, Longitude = NULL, City = NULL, Country = NULL, PostalCode = NULL, Power = NULL, Powertype = NULL) {
  #Apply the filters to keep the data usage lower
  rows <- nrow(EVSE_df)
  if (!is.null(Latitude))   EVSE_df <- EVSE_df[EVSE_df$latitude %in% Latitude, ]
  if (!is.null(Longitude))  EVSE_df <- EVSE_df[EVSE_df$longitude %in% Longitude, ]
  if (!is.null(City))       EVSE_df <- EVSE_df[EVSE_df$City %in% City, ]
  if (!is.null(Country))    EVSE_df <- EVSE_df[EVSE_df$Country %in% Country, ]
  if (!is.null(PostalCode)) EVSE_df <- EVSE_df[EVSE_df$PostalCode %in% PostalCode, ]
  if (!is.null(Power))      EVSE_df <- EVSE_df[EVSE_df$ChargingFacility_power %in% Power, ]
  if (!is.null(Powertype))  EVSE_df <- EVSE_df[EVSE_df$ChargingFacility_powertype %in% Powertype, ]
  rows_elim <- rows_elim + (rows - nrow(EVSE_df))  #total number of rows that have been eliminated by the filters.
  
  cat("Rows eliminated from filters: ",rows_elim, "\n")
  return(EVSE_df)
}

```

# Charging point availability

```{r load and append ALL Parquet files into a df}
#this gives a VERY LONG dataframe.
load_charging_df <- function(files_to_load, existing_df = NULL) {
  for(file in files_to_load) {
    this_df <- arrow::read_parquet(file)
    #check if the data are already in the working dataframe
    if(!is.null(existing_df)) {
      if(!exists("return_df")) return_df <- existing_df
      check_datetime <- as.POSIXct(this_df$datetime[1], format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich")
      existing_df$datetime <- as.POSIXct(existing_df$datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich")
      if(check_datetime %in% existing_df$datetime) next #If the date already exists in the existing dataframe, we do not need to load the same data again.
    }
    #reformat the columns and select only the ones wanted
    this_df <- this_df %>%
      transmute(datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
                EvseID = as.factor(EvseID),
                EVSEStatus = as.factor(EVSEStatus),
                OperatorID = as.factor(OperatorID),
                OperatorName = as.factor(OpeartorName))
    #append this to the existing dataframe if it already exists
    if(!exists("return_df")) return_df <- this_df else return_df <- rbind(return_df, this_df)
    
    rm(this_df)
  }
  return(return_df)
}

load_charging_dt <- function(files_to_load, existing_dt = NULL) {
  # Ensure existing_dt is a data.table
  if (!is.null(existing_dt)) {
    existing_dt <- as.data.table(existing_dt)[, .(
      datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
      EvseID = as.factor(EvseID),
      EVSEStatus = as.factor(EVSEStatus),
      OperatorID = as.factor(OperatorID),
      OperatorName = as.factor(OpeartorName)
    )]
  }
  
  dt_list <- list()
  
  for(file in files_to_load) {
    this_dt <- as.data.table(arrow::read_parquet(file))[, .(
      datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
      EvseID = as.factor(EvseID),
      EVSEStatus = as.factor(EVSEStatus),
      OperatorID = as.factor(OperatorID),
      OperatorName = as.factor(OpeartorName)
    )]
    
    #check if the data are already in the working dataframe
    if(!is.null(existing_dt)) {
      if (this_dt$datetime[1] %in% existing_dt$datetime) {
        message("Skipping file (already loaded): ", file)
        next
    
      }
    }
    dt_list[[length(dt_list) + 1]] <- this_dt
  }

  # Combine all new data.tables
  if (length(dt_list) > 0) {
    new_dt <- rbindlist(dt_list, use.names = TRUE, fill = TRUE)
    # Combine with existing_dt if provided
    if (!is.null(existing_dt)) {
      return(rbindlist(list(existing_dt, new_dt), use.names = TRUE, fill = TRUE))
      } else {
        return(new_dt)
        }
    } else {
      # No new files to add
      return(existing_dt)
      }
  
  }


```


```{r function for loading data}
# Function to read and filter one file
process_file <- function(file, IDs = NULL, status = NULL, year = NULL, month = NULL, day = NULL, wday = NULL, hour = NULL, minute = NULL, existing_dt = NULL) {
  dt <- as.data.table(arrow::read_parquet(file))

  if (nrow(dt) == 0 || !"datetime" %in% names(dt)) return(NULL)

  dt <- dt[, .(
    datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
    EvseID = as.factor(EvseID),
    EVSEStatus = as.factor(EVSEStatus)
  )]
  rows_start <- nrow(dt)

  # Skip already loaded
  if (!is.null(existing_dt) && dt$datetime[1] %in% existing_dt$datetime) return(NULL)

  # Apply filters
  if (!is.null(IDs))    dt <- dt[EvseID %in% IDs, ]
  if (!is.null(status)) dt <- dt[EVSEStatus %in% status, ]
  if (!is.null(year))   dt <- dt[year(datetime)   %in% year, ]
  if (!is.null(month))  dt <- dt[month(datetime)  %in% month, ]
  if (!is.null(day))    dt <- dt[mday(datetime)   %in% day, ]
  if (!is.null(wday))   dt <- dt[lubridate::wday(datetime, week_start = 1) %in% wday, ]
  if (!is.null(hour))   dt <- dt[hour(datetime)   %in% hour, ]
  if (!is.null(minute)) dt <- dt[minute(datetime) %in% minute, ]
  
  rows_elim <- rows_start - nrow(dt)

  # return(dt)
  return(list(dt = dt, rows_elim = rows_elim))
}
```



```{r function for loading data using parallelization}
#load and filter data to keep objects small, but together
load_filter_charging_dt_parallel <- function(files_to_load, IDs = NULL, status = c("Available","Occupied"), year = NULL, month = NULL, day = NULL, wday = NULL, hour = NULL, minute = NULL, existing_dt = NULL, n_cores = 4, mem_efficient = FALSE) {
  rows_elim <- 0
  file_summaries <- list()
  # Ensure existing_dt is a data.table
  if (!is.null(existing_dt)) {
    existing_dt <- as.data.table(existing_dt)[, .(
      datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
      EvseID = as.factor(EvseID),
      EVSEStatus = as.factor(EVSEStatus)
      # OperatorID = as.factor(OperatorID),
      # OperatorName = as.factor(OpeartorName)
    )]
    file_summaries[[length(file_summaries) + 1]] <- summary(this_dt)
    #Apply the filters to keep the data usage lower
    rows <- nrow(existing_dt)
    if (!is.null(IDs))    existing_dt <- existing_dt[existing_dt$EvseID %in% IDs, ]
    if (!is.null(status)) existing_dt <- existing_dt[existing_dt$EVSEStatus %in% status, ]
    if (!is.null(year))   existing_dt <- existing_dt[year(datetime)   %in% year, ]
    if (!is.null(month))  existing_dt <- existing_dt[month(datetime)  %in% month, ]
    if (!is.null(day))    existing_dt <- existing_dt[day(datetime)   %in% day, ]
    if (!is.null(wday))   existing_dt <- existing_dt[lubridate::wday(datetime, week_start = 1) %in% wday, ] #Monday = 1, Sunday = 7
    if (!is.null(hour))   existing_dt <- existing_dt[hour(datetime)   %in% hour, ]
    if (!is.null(minute)) existing_dt <- existing_dt[minute(datetime) %in% minute, ]
    rows_elim <-  rows - nrow(existing_dt)
  }
  
  #Pre-filters of files_to_load if a year or month is given
  if(!is.null(year)) {
    year_patterns <- paste0(year, collapse = "|")   # e.g. "2024|2023"
    # --- Filter logic: keep if any year OR month appears ---
    files_to_load <- files_to_load[grepl(year_patterns, files_to_load)]
  }
  if(!is.null(month)) {
    month_strs <- sprintf("%02d", month)
    month_patterns <- paste0("-", month_strs, "_", collapse = "|")  # e.g. "-09_|-12_"
    files_to_load <- files_to_load[grepl(month_patterns, files_to_load)]
  }

  # Create cluster
  n_cores <- ifelse(n_cores <= 0, detectCores() + n_cores, n_cores) #if a negative number is entered, that is to be the number of cores UNUSED by this execution
  cat("Executing on ",n_cores, " cores.\n")
  cl <- makeCluster(n_cores)
  clusterEvalQ(cl, {
    library(data.table)
    library(arrow)
    library(lubridate)
  })
  clusterExport(cl, varlist = c("files_to_load","IDs","status","year","month",
                                "day","wday","hour","minute","existing_dt","process_file"),
                envir = environment())
  
  # Run in parallel
  dt_list <- parLapply(cl, files_to_load, process_file,
                       IDs = IDs, 
                       status = status, 
                       year = year, 
                       month = month, 
                       day = day, 
                       wday = wday, 
                       hour = hour, 
                       minute = minute, 
                       existing_dt = existing_dt)
  stopCluster(cl)
  
  # Remove NULL results (skipped files)
  dt_list <- Filter(Negate(is.null), dt_list)

  # Combine all new data.tables
  if (length(dt_list) > 0) {
    # Extract dt objects and eliminated rows
    all_dt    <- lapply(dt_list, `[[`, "dt")
    rows_elim <- sum(sapply(dt_list, `[[`, "rows_elim"), na.rm = TRUE)
    
    cat("Rows eliminated from filters: ",rows_elim, "\n")
    
    # Combine all new data.tables
    if(mem_efficient == FALSE) {
      new_dt <- rbindlist(all_dt, use.names = TRUE, fill = TRUE)
    } else {
      # Streamed incremental rbindlist (low-memory binding)
      cat("Combining", length(all_dt), "tables incrementally...\n")
      new_dt <- NULL
      for (i in seq_along(all_dt)) {
        chunk <- all_dt[[i]]
        if (is.null(new_dt)) {
          new_dt <- chunk
        } else {
          new_dt <- rbindlist(list(new_dt, chunk), use.names = TRUE, fill = TRUE)
        }
        rm(chunk)
        if (i %% 5 == 0 || i == length(all_dt)) {
          cat("  Bound", i, "of", length(all_dt), "files...\n")
        }
        gc(verbose = FALSE)
      }
      
    }
    
    # Combine with existing_dt if provided
    if (!is.null(existing_dt)) {
      return(rbindlist(list(existing_dt, new_dt), use.names = TRUE, fill = TRUE))
      } else {
        return(new_dt)
        }
    } else {
      # No new files to add
      return(existing_dt)
      }
  
  }

```

```{r Duplicate working function - no Parallelization}
#load and filter data to keep objects small, but together
load_filter_charging_dt <- function(files_to_load, IDs = NULL, status = c("Available","Occupied"), year = NULL, month = NULL, day = NULL, wday = NULL, hour = NULL, minute = NULL, existing_dt = NULL) {
  rows_elim <- 0
  file_summaries <- list()
  # Ensure existing_dt is a data.table
  if (!is.null(existing_dt)) {
    existing_dt <- as.data.table(existing_dt)[, .(
      datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
      EvseID = as.factor(EvseID),
      EVSEStatus = as.factor(EVSEStatus)
      # OperatorID = as.factor(OperatorID),
      # OperatorName = as.factor(OpeartorName)
    )]
    file_summaries[[length(file_summaries) + 1]] <- summary(this_dt)
    #Apply the filters to keep the data usage lower
    rows <- nrow(existing_dt)
    if (!is.null(IDs))    existing_dt <- existing_dt[existing_dt$EvseID %in% IDs, ]
    if (!is.null(status)) existing_dt <- existing_dt[existing_dt$EVSEStatus %in% status, ]
    if (!is.null(year))   existing_dt <- existing_dt[year(datetime)   %in% year, ]
    if (!is.null(month))  existing_dt <- existing_dt[month(datetime)  %in% month, ]
    if (!is.null(day))    existing_dt <- existing_dt[mday(datetime)   %in% day, ]
    if (!is.null(wday))   existing_dt <- existing_dt[lubridate::wday(datetime, week_start = 1) %in% wday, ] #Monday = 1, Sunday = 7
    if (!is.null(hour))   existing_dt <- existing_dt[hour(datetime)   %in% hour, ]
    if (!is.null(minute)) existing_dt <- existing_dt[minute(datetime) %in% minute, ]
    rows_elim <-  rows - nrow(existing_dt)
  }
  
  dt_list <- list()
  
  for(file in files_to_load) {
    this_dt <- as.data.table(arrow::read_parquet(file))

    # Skip empty files or files without the required column
    if (nrow(this_dt) == 0 || !"datetime" %in% names(this_dt)) {
      message("Skipping file (empty or missing 'datetime'): ", file)
      next
    }
    
    # Now safely transform the columns
    this_dt <- this_dt[, .(
      datetime = as.POSIXct(datetime, format = "%Y-%m-%d %H:%M:%S", tz = "Europe/Zurich"),
      EvseID = as.factor(EvseID),
      EVSEStatus = as.factor(EVSEStatus)
    )]
    #check if the data are already in the working dataframe
    if(!is.null(existing_dt)) {
      if (this_dt$datetime[1] %in% existing_dt$datetime) {
        message("Skipping file (already loaded): ", file)
        next
    
      }
    }
    #Apply the filters to keep the data usage lower
    rows <- nrow(this_dt)
    if (!is.null(IDs))    this_dt <- this_dt[this_dt$EvseID %in% IDs, ]
    if (!is.null(status)) this_dt <- this_dt[this_dt$EVSEStatus %in% status, ]
    if (!is.null(year))   this_dt <- this_dt[year(datetime)   %in% year, ]
    if (!is.null(month))  this_dt <- this_dt[month(datetime)  %in% month, ]
    if (!is.null(day))    this_dt <- this_dt[mday(datetime)   %in% day, ]
    if (!is.null(wday))   this_dt <- this_dt[lubridate::wday(datetime, week_start = 1) %in% wday, ] #Monday = 1, Sunday = 7
    if (!is.null(hour))   this_dt <- this_dt[hour(datetime)   %in% hour, ]
    if (!is.null(minute)) this_dt <- this_dt[minute(datetime) %in% minute, ]
    rows_elim <- rows_elim + (rows - nrow(this_dt))  #total number of rows that have been eliminated by the filters.
    
    #Add it to the running list of new dts that should be merged
    dt_list[[length(dt_list) + 1]] <- this_dt
  }
  
  cat("Rows eliminated from filters: ",rows_elim, "\n")

  # Combine all new data.tables
  if (length(dt_list) > 0) {
    new_dt <- rbindlist(dt_list, use.names = TRUE, fill = TRUE)
    # Combine with existing_dt if provided
    if (!is.null(existing_dt)) {
      return(rbindlist(list(existing_dt, new_dt), use.names = TRUE, fill = TRUE))
      } else {
        return(new_dt)
        }
    } else {
      # No new files to add
      return(existing_dt)
      }
  
  }
```


```{r Get number of rows in the total dataset}
system.time({
  chg_dt_Rows <- load_filter_charging_dt_parallel(chg_avail_files,
                                                  status = NULL,
                                                  year = NULL,
                                                  n_cores = 4,
                                                  mem_efficient = TRUE)
})
summary(chg_dt_Rows)
```



```{r generate a single typical day set of data for a selected power level of charger}
system.time({
  # summary(mid_power_EVSE)

  low_power_EVSE <- CH_EVSE_details[!is.na(CH_EVSE_details$ChargingFacility_power) &
                                    CH_EVSE_details$ChargingFacility_power < 22, ]

  mid_power_EVSE <- CH_EVSE_details[!is.na(CH_EVSE_details$ChargingFacility_power) &
                                    CH_EVSE_details$ChargingFacility_power >= 22 &
                                    CH_EVSE_details$ChargingFacility_power < 50, ]
  
  high_power_EVSE <- CH_EVSE_details[!is.na(CH_EVSE_details$ChargingFacility_power) &
                                    CH_EVSE_details$ChargingFacility_power > 50, ]
  # rm(chg_dt_full)
  filter_IDs <- unique(high_power_EVSE$EvseID)
  # undebug(load_filter_charging_dt_parallel)
  chg_dt_full <- load_filter_charging_dt_parallel(chg_avail_files,
                                                 IDs = filter_IDs,
                                                 year = NULL, 
                                                 month = c(5,6,7), #summer 
                                                 day = NULL, 
                                                 wday = c(1,2,3,4,5), #weekday 
                                                 hour = NULL, 
                                                 minute = NULL, 
                                                 existing_dt = NULL,
                                                 n_cores = 8)
})
print(format(object.size(chg_dt_full),units = "GB"))
```

## Exploratory analysis of the charging dataset
```{r function to check how many times each status appears for each EvseID}
summarize_evse_status <- function(dt_statuses, id_col = "EvseID", status_col = "EVSEStatus") {
  # Ensure it's a data.table
  if(is.data.table(dt_statuses) == FALSE) dt_statuses <- as.data.table(dt_statuses)
  
  # Step 1: Count occurrences of each status per EvseID
  # counts <- dt_statuses[, .N, by = .(EvseID = get(id_col), status = get(status_col))]
  dt_statuses[, c("EvseID","EVSEStatus") := .(as.character(EvseID), as.character(EVSEStatus))]
  gc()
  counts <- dt_statuses[, .N, by = .(EvseID, EVSEStatus)]
  
  # Step 2: Reshape to wide format
  summary_dt <- dcast(
    counts,
    formula = EvseID ~ EVSEStatus,
    value.var = "N",
    fill = 0
  )

  # Step 3: Add Total column (sum across all status columns)
  setDT(summary_dt)
  status_cols <- setdiff(names(summary_dt), "EvseID")
  summary_dt[, Total := rowSums(.SD), .SDcols = status_cols]
  
  # Step 4 (optional): Order columns nicely
  setcolorder(summary_dt, c("EvseID", sort(status_cols), "Total"))
  
  return(as.data.table(summary_dt))
}
```

```{r }
if(exists("chg_dt_full")) rm(chg_dt_full) #clear the dt if it's there
system.time({
  chg_dt_full <- load_filter_charging_dt_parallel(chg_avail_files,
                                                # status = NULL,
                                                # year = 2025,
                                                month = c(5,6,7),
                                                wday = c(1,2,3,4,5),
                                                hour = 12,
                                                mem_efficient = TRUE)
})

# undebug(summarize_evse_status)
chg_dt_summary <- summarize_evse_status(chg_dt_full)

summary_dt <- chg_dt_full[ , .(prop_occupied = mean(EVSEStatus == "Occupied")), by = datetime]
summary_dt <- summary_dt[, hour := as.integer(format(datetime, "%H"))]

```



## Utilization rate of charge poitns (EVSE IDs)
```{r function to get the utilization rate of each collected timestamp}
utilization_rate <- function(dt_input, IDs = NULL, year = NULL, month = NULL, day = NULL, wday = NULL, hour = NULL, minute = NULL) { 
  '
  Input: Takes the dt object from function "load_filter_charging_dt()" as the input
  Operation: Applied additional filters based on what could be wanted, calulates the proportion of occupied charge points among those available among the charging points desired for the timeframe desired
  Output: a single value as the proportion of occupied vs occupied + available for all the rows in the input dt.
  '
  if (!is.null(IDs))    dt_input <- dt_input[dt_input$EvseID %in% IDs, ]
  if (!is.null(year))   dt_input <- dt_input[year(datetime)   %in% year, ]
  if (!is.null(month))  dt_input <- dt_input[month(datetime)  %in% month, ]
  if (!is.null(day))    dt_input <- dt_input[day(datetime)   %in% day, ]
  if (!is.null(wday))   dt_input <- dt_input[lubridate::wday(datetime, week_start = 1) %in% wday, ] #Monday = 1, Sunday = 7
  if (!is.null(hour))   dt_input <- dt_input[hour(datetime)   %in% hour, ]
  if (!is.null(minute)) dt_input <- dt_input[minute(datetime) %in% minute, ]
  
  percent_occupied <- mean(dt_input$EVSEStatus == "Occupied")
  
  return(percent_occupied)
  
}

```


## Functions for computing utilization rate statistics (hourly distributions for the days given)
```{r direct computation of all data}
#compute the utilization rate stats for one vector.
compute_stats <- function(x) {
  # Ensure input is numeric
  x <- as.numeric(x)
  
  # Compute summary statistics
  min_prop    <- min(x, na.rm = TRUE)
  q1_prop     <- as.numeric(quantile(x, 0.25, na.rm = TRUE))
  median_prop <- median(x, na.rm = TRUE)
  mean_prop   <- mean(x, na.rm = TRUE)
  q3_prop     <- as.numeric(quantile(x, 0.75, na.rm = TRUE))
  max_prop    <- max(x, na.rm = TRUE)
  sd_prop     <- sd(x, na.rm = TRUE)
  
  # 95% CI using standard formula (mean ± 1.96*sd/sqrt(n))
  n <- sum(!is.na(x))
  ci95_low  <- mean_prop - 1.96 * sd_prop / sqrt(n)
  ci95_high <- mean_prop + 1.96 * sd_prop / sqrt(n)
  
  # Return as named list
  return(list(
    N           = n,
    min_prop    = min_prop,
    q1_prop     = q1_prop,
    median_prop = median_prop,
    mean_prop   = mean_prop,
    q3_prop     = q3_prop,
    max_prop    = max_prop,
    sd_prop     = sd_prop,
    ci95_low    = ci95_low,
    ci95_high   = ci95_high
  ))
}
```


```{r Aggregated means for each hour of the day (N = number of days in the input dt)}
#Aggregating single hour observation across days
daily_hourly_summary <- function(dt) {
  # # Expect dt to have: datetime (POSIXct) and prop_occupied (numeric)
  # dt <- as.data.table(dt)
  
  # Extract date and hour from POSIX datetime
  dt[, `:=`(
    date = as.Date(datetime),
    hour = as.integer(format(datetime, "%H"))
  )]
  
  # Compute daily mean utilization for each hour
  daily_hour <- dt[, .(daily_mean = mean(prop_occupied, na.rm = TRUE)),
                   by = .(date, hour)]
  
  # Compute summary statistics across days for each hour
  summary_by_hour <- daily_hour[, .(
    N_days    = .N,
    min_prop       = min(daily_mean, na.rm = TRUE),
    q1_prop       = quantile(daily_mean, 0.25, na.rm = TRUE),
    median_prop    = median(daily_mean, na.rm = TRUE),
    mean_prop      = mean(daily_mean, na.rm = TRUE),
    q3_prop       = quantile(daily_mean, 0.75, na.rm = TRUE),
    max_prop       = max(daily_mean, na.rm = TRUE),
    sd_prop        = sd(daily_mean, na.rm = TRUE),
    ci95_low    = {
      m <- mean(daily_mean, na.rm = TRUE)
      s <- sd(daily_mean, na.rm = TRUE)
      n <- .N
      m - 1.96 * s / sqrt(n)
    },
    ci95_high   = {
      m <- mean(daily_mean, na.rm = TRUE)
      s <- sd(daily_mean, na.rm = TRUE)
      n <- .N
      m + 1.96 * s / sqrt(n)
    }
  ), by = hour]
  
  setorder(summary_by_hour, hour)
  return(summary_by_hour)
}

daily_mean_sample <- function(dt, hr) {
  # # Expect dt to have: datetime (POSIXct) and prop_occupied (numeric)
  # dt <- as.data.table(dt)
  
  # Extract date and hour from POSIX datetime
  dt[, `:=`(
    date = as.Date(datetime),
    hour = as.integer(format(datetime, "%H"))
  )]
  
  # Compute daily mean utilization for each hour
  daily_hour <- dt[, .(daily_mean = mean(prop_occupied, na.rm = TRUE)),
                   by = .(date, hour)]
  
  mean_hour_sample <- daily_hour[hour == hr, daily_mean]
  
  return(mean_hour_sample)
}
  
```

```{r Take maximum value as the hourly value when aggregating across days}
#Aggregating single hour observation across days
daily_hourly_summary_max <- function(dt) {
  # # Expect dt to have: datetime (POSIXct) and prop_occupied (numeric)
  # dt <- as.data.table(dt)
  
  # Extract date and hour from POSIX datetime
  dt[, `:=`(
    date = as.Date(datetime),
    hour = as.integer(format(datetime, "%H"))
  )]
  
  # Compute daily max utilization for each hour
  daily_hour <- dt[, .(daily_max = max(prop_occupied, na.rm = TRUE)),
                   by = .(date, hour)]
  
  # Compute summary statistics across days for each hour
  summary_by_hour <- daily_hour[, .(
    N_days    = .N,
    min_prop       = min(daily_max, na.rm = TRUE),
    q1_prop       = quantile(daily_max, 0.25, na.rm = TRUE),
    median_prop    = median(daily_max, na.rm = TRUE),
    mean_prop      = mean(daily_max, na.rm = TRUE),
    q3_prop       = quantile(daily_max, 0.75, na.rm = TRUE),
    max_prop       = max(daily_max, na.rm = TRUE),
    sd_prop        = sd(daily_max, na.rm = TRUE),
    ci95_low    = {
      m <- mean(daily_max, na.rm = TRUE)
      s <- sd(daily_max, na.rm = TRUE)
      n <- .N
      m - 1.96 * s / sqrt(n)
    },
    ci95_high   = {
      m <- mean(daily_max, na.rm = TRUE)
      s <- sd(daily_max, na.rm = TRUE)
      n <- .N
      m + 1.96 * s / sqrt(n)
    }
  ), by = hour]
  
  setorder(summary_by_hour, hour)
  return(summary_by_hour)
}

daily_max_sample <- function(dt, hr) {
  # # Expect dt to have: datetime (POSIXct) and prop_occupied (numeric)
  # dt <- as.data.table(dt)
  
  # Extract date and hour from POSIX datetime
  dt[, `:=`(
    date = as.Date(datetime),
    hour = as.integer(format(datetime, "%H"))
  )]
  
  # Compute daily mean utilization for each hour
  daily_hour <- dt[, .(daily_max = max(prop_occupied, na.rm = TRUE)),
                   by = .(date, hour)]
  
  max_hour_sample <- daily_hour[hour == hr, daily_max]
  
  return(max_hour_sample)
}
```


```{r Bootstrapped from hourly observations}
# Function to compute percentile-based CI for the mean
compute_percentile_CI <- function(x, n_boot) {
  boot_means <- replicate(n_boot, mean(sample(x, replace = TRUE)))
  ci <- quantile(boot_means, probs = c(0.025, 0.975), na.rm = TRUE)
  return(ci)
}
  
##Using bootstrapping
bootstrapped_hourly_summary <- function(dt, n_boot = 1000, seed = 123, sample.ret = FALSE) {
  set.seed(seed)
  
  # # Ensure input is a data.table
  # dt <- as.data.table(dt)
  
  # Extract hour from datetime
  dt[, hour := as.integer(format(datetime, "%H"))]
  
  # choose n_boot to be equal to the size of the original sample if not given
  n_samp <- uniqueN(dt$date)
  if(is.null(n_boot)) n_boot <- n_days
  
  # Bootstrapped summary by hour
  result <- dt[, {
    # Bootstrap: sample prop_occupied with replacement n_boot times
    boot_stats <- replicate(n_boot, unlist(compute_stats(sample(prop_occupied, replace = TRUE))))
    
    # Average statistics across bootstrap samples
    stats_mean <- rowMeans(boot_stats)
    
    # Percentile-based 95% CI for the mean of prop_occupied
    ci95 <- compute_percentile_CI(prop_occupied, n_boot)
    
    # Combine results
    stats_mean["ci95_low"]  <- ci95[1]
    stats_mean["ci95_high"] <- ci95[2]
    
    as.list(stats_mean)
  }, by = hour]
  
  # Sort by hour
  setorder(result, hour)
  
  if(sample.ret == FALSE) return(result)
    
  #If a sample is requested
  first_hour <- result$hour[1]
  values_first_hour <- dt[hour == first_hour, prop_occupied]
  
  # Generate bootstrap sample means
  boot_sample <- replicate(n_boot, mean(sample(values_first_hour, replace = TRUE)))
  
  # Return BOTH summary and sample in a list
  return(list(
    summary = result,
    sample_hour = list(
      hour = first_hour,
      boot_samples = boot_sample
    )
  ))
}

bootstrapped_hourly_samples <- function(dt, hr, n_boot = 1000, seed = 123) {
  set.seed(seed)

  dt <- as.data.table(dt)

  # Extract hour
  dt[, hour := as.integer(format(datetime, "%H"))]

  values_hour <- dt[hour == hr, prop_occupied]
  
  # Generate bootstrap sample 
  boot_sample <- sample(values_hour, size = n_boot, replace = TRUE)

  return(boot_sample)
}

```


```{r Hierarchal (cluster) boostrapping. Resampling is done from days, not individual 5-minute observations}

hierarchical_hourly_bootstrap <- function(dt, n_boot = NULL, seed = 123) {
  set.seed(seed)
  dt <- as.data.table(dt)
  
  # Extract date and hour
  dt[, `:=`(
    date = as.Date(datetime),
    hour = as.integer(format(datetime, "%H"))
  )]
  
  # Compute daily means by hour (one per day-hour)
  daily_hour <- dt[, .(daily_mean = mean(prop_occupied, na.rm = TRUE)),
                   by = .(date, hour)]
  
  # Prepare list of daily mean vectors per hour
  hour_list <- split(daily_hour, by = "hour", keep.by = FALSE)
  
  # choose n_boot to be equal to the size of the original sample if not given
  n_days <- uniqueN(daily_hour$date)
  if(is.null(n_boot)) n_boot <- n_days

  # Hierarchical bootstrapped summary by hour
  result <- daily_hour[, {
    days <- unique(date)
    n_days <- length(days)
    current_hour <- unique(hour)
    
    # Hierarchical bootstrap. Sampled over days.
    boot_stats <- replicate(n_boot, {
      sampled_days <- sample(days, size = n_days, replace = TRUE)
      sampled_data <- daily_hour[date %in% sampled_days & hour %in% current_hour]
      unlist(compute_stats(sampled_data$daily_mean))
    })
    
    # Average statistics across bootstrap samples
    stats_mean <- rowMeans(boot_stats, na.rm = TRUE)
    
    # Percentile-based CI
    ci95 <- compute_percentile_CI(daily_mean[hour == current_hour], n_boot)
    stats_mean["ci95_low"]  <- ci95[1]
    stats_mean["ci95_high"] <- ci95[2]
    
    as.list(stats_mean)
  }, by = hour]
  
  # Sort by hour
  setorder(result, hour)
  
  return(result)
}



h_bootstrapped_hourly_samples <- function(dt, hr, n_boot = NULL, seed = 123) {
  set.seed(seed)

  dt <- as.data.table(dt)
  
  # Extract date and hour
  dt[, `:=`(
    date = as.Date(datetime),
    hour = as.integer(format(datetime, "%H"))
  )]
  
  # Compute daily means by hour (one per day-hour)
  daily_hour <- dt[, .(daily_mean = mean(prop_occupied, na.rm = TRUE)),
                   by = .(date, hour)]
  
  n_days <- uniqueN(daily_hour$date)
  if(is.null(n_boot)) n_boot <- n_days

  values_hour <- daily_hour[hour == hr, daily_mean]
  
  # Generate bootstrap sample means
  boot_sample <-sample(values_hour, size = n_boot, replace = TRUE)
  
  return(boot_sample)
}

```

```{r Sample distributions of one hour}
crosssection <- summary_dt[hour == 12, prop_occupied]
mean_hourly <- daily_mean_sample(summary_dt, hr = 12)
max_hourly <- daily_max_sample(summary_dt, hr = 12)
bootstrapped <- bootstrapped_hourly_samples(summary_dt, hr = 12)
hbootstrapped <- h_bootstrapped_hourly_samples(summary_dt, hr = 12)

#cross sectional
length(crosssection)
hist(crosssection, xlim = c(0, 0.2), breaks = "FD")
#hourly means
length(mean_hourly)
hist(mean_hourly, xlim = c(0, 0.2), breaks = "FD")
#hourly maxes
length(max_hourly)
hist(max_hourly, xlim = c(0, 0.2), breaks = "FD")
#bootstrapped
length(bootstrapped)
hist(bootstrapped, xlim = c(0, 0.2), breaks = "FD")
#hBootstrapped
length(hbootstrapped)
hist(hbootstrapped, xlim = c(0, 0.2), breaks = "FD")

#QQ plots of the cross sectional and sampled values
qqnorm(crosssection, pch = 19, frame = FALSE, main = "Cross-sectional Normal Q-Q Plot")
qqline(crosssection, col = 'lightblue', lwd = 2)

qqnorm(mean_hourly, pch = 19, frame = FALSE, main = "Hourly Mean Normal Q-Q Plot")
qqline(mean_hourly, col = 'lightblue', lwd = 2)

qqnorm(bootstrapped, pch = 19, frame = FALSE, main = "Bootstrapped Normal Q-Q Plot")
qqline(bootstrapped, col = 'lightblue', lwd = 2)

qqnorm(hbootstrapped, pch = 19, frame = FALSE, main = "H Bootstrapped Normal Q-Q Plot")
qqline(hbootstrapped, col = 'lightblue', lwd = 2)


```

```{r comparing the resulting summary statistics for the hourly utilization rates}
hourly_summary <- summary_dt[, compute_stats(prop_occupied), by = hour]
#undebug(bootstrapped_hourly_summary)
hourly_summary_boot <- bootstrapped_hourly_summary(summary_dt, n_boot = 1000)
hourly_summary_agg <- daily_hourly_summary(summary_dt)
hourly_summary_agg_max <- daily_hourly_summary_max(summary_dt)
hourly_summary_hboot <- hierarchical_hourly_bootstrap(summary_dt)


hourly_compare <- hourly_summary_agg_max - hourly_summary_agg

```


```{r visualize a utilization curve}
utilization_visual <- function(dt_util,
                               min = TRUE,
                               max = TRUE,
                               mean = TRUE,
                               ci95_low = TRUE,
                               ci95_high = TRUE,
                               q1 = FALSE,
                               median = FALSE,
                               q3 = FALSE,
                               sd = FALSE,
                               line = TRUE,
                               title = "Hourly Utilization Distribution") {
  # --- Defensive checks ---
  if (!"hour" %in% names(dt_util)) stop("Data must have an 'hour' column.")
  
  # Ensure hour is ordered (0–23)
  dt_util <- dt_util %>% arrange(hour)
  
  # --- Reshape data to long form for flexible plotting ---
  df_long <- dt_util %>%
    pivot_longer(
      cols = c("min_prop", "q1_prop", "median_prop", "mean_prop", "q3_prop",
               "max_prop", "sd_prop", "ci95_low", "ci95_high"),
      names_to = "metric",
      values_to = "value",
      values_drop_na = TRUE
    )
  
  # --- Determine which metrics to include ---
  metrics_to_show <- c()
  if (min)        metrics_to_show <- c(metrics_to_show, "min_prop")
  if (max)        metrics_to_show <- c(metrics_to_show, "max_prop")
  if (mean)       metrics_to_show <- c(metrics_to_show, "mean_prop")
  if (ci95_low)   metrics_to_show <- c(metrics_to_show, "ci95_low")
  if (ci95_high)  metrics_to_show <- c(metrics_to_show, "ci95_high")
  if (q1)         metrics_to_show <- c(metrics_to_show, "q1_prop")
  if (median)     metrics_to_show <- c(metrics_to_show, "median_prop")
  if (q3)         metrics_to_show <- c(metrics_to_show, "q3_prop")
  if (sd)         metrics_to_show <- c(metrics_to_show, "sd_prop")
  
  df_long <- df_long %>% filter(metric %in% metrics_to_show)
  
  # --- Detect available sample size column ---
  n_col <- NULL
  n_label <- NULL
  if ("N" %in% names(dt_util)) {
    n_col <- "N"
    n_label <- "observations for each hour"
  } else if ("N_days" %in% names(dt_util)) {
    n_col <- "N_days"
    n_label <- "days aggregated for each hour"
  }
  
  # --- Plotting ---
  if (line) {
    # Line plot of selected summary metrics
    p <- ggplot(df_long, aes(x = hour, y = value, color = metric)) +
      geom_line(linewidth = 1) +
      geom_point(size = 1.8) +
      labs(
        x = "Hour of Day",
        y = "Proportion",
        title = title
      ) +
      theme_minimal(base_size = 13) +
      theme(
        plot.title = element_text(face = "bold"),
        legend.title = element_blank()
      )
  } else {
    # Boxplot visualization from quantiles if available
    p <- ggplot(dt_util, aes(x = factor(hour))) +
      geom_boxplot(
        aes(
          ymin = min_prop,
          lower = q1_prop,
          middle = median_prop,
          upper = q3_prop,
          ymax = max_prop
        ),
        stat = "identity",
        fill = "skyblue",
        alpha = 0.4
      ) +
      labs(
        x = "Hour of Day",
        y = "Proportion",
        title = title
      ) +
      theme_minimal(base_size = 13) +
      theme(
        plot.title = element_text(face = "bold"))
        
    # --- Add annotation about sample size ---
    if (!is.null(n_col)) {
      n_values <- dt_util[[n_col]]
      n_min <- min(n_values, na.rm = TRUE)
      n_max <- max(n_values, na.rm = TRUE)
      
      if (n_min == n_max) {
        note_text <- paste0("n ", n_label, " in each boxplot = ", n_min)
      } else {
        note_text <- paste0("Range of n ", n_label, " in each boxplot = ", n_min, " - ", n_max)
      }
      
      # Position text in top-left corner
      y_top <- max(dt_util$max_prop - 0.02, na.rm = TRUE)
      x_min <- min(as.numeric(as.character(dt_util$hour)) + 1, na.rm = TRUE)
      
      p <- p +
        annotate(
          "text",
          x = x_min,
          y = y_top,
          label = note_text,
          hjust = 0,
          vjust = -0.8,
          size = 3.5,
          fontface = "italic"
        )
    }
  }
  
  return(p)
}

```

```{r visualization trials}
utilization_visual(hourly_summary_agg_max, title = "Hourly distibution of maximum utilization rates")
utilization_visual(hourly_summary_agg_max, line = FALSE, title = "Hourly distibution of maximum utilization rates")
utilization_visual(hourly_summary_agg, title = "Hourly distribution of mean utilization rates")
utilization_visual(hourly_summary_agg, line = FALSE, title = "Hourly distribution of mean utilization rates")
utilization_visual(hourly_summary_boot, title = "Hourly bootstrapped distribution of mean utilization rates")
utilization_visual(hourly_summary_boot, line = FALSE, title = "Hourly bootstrapped distribution of mean utilization rates")
```


## Cross sectional utilization rate of all data
```{r}
# Dataset is large so it must be done by hour as R cannot allocate an object of the size needed.
hours <- 0:23

# Initialize lists to store results per hour
stats_list       <- vector("list", length(hours))
boot_list        <- vector("list", length(hours))
agg_list         <- vector("list", length(hours))
agg_max_list     <- vector("list", length(hours))
hboot_list       <- vector("list", length(hours))

for (h in hours) {
  cat("Processing hour", h, "...\n")
  
  # --- Load data filtered for this hour ---
  dt_hour <- load_filter_charging_dt_parallel(
    chg_avail_files,
    IDs = NULL,
    year = NULL,
    month = NULL,
    day = NULL,
    wday = NULL,
    hour = h, # filter by this hour
    minute = NULL,
    existing_dt = NULL,
    n_cores = 4,
    mem_efficient = TRUE)
  
  # If no rows returned, create NA row
  if (nrow(dt_hour) == 0) {
    placeholder <- data.table(matrix(NA_real_, nrow = 1, ncol = 11))
    colnames(placeholder) <- c("N","min_prop","q1_prop","median_prop","mean_prop",
                               "q3_prop","max_prop","sd_prop","ci95_low","ci95_high","hour")
    stats_list[[h + 1]]   <- copy(placeholder)
    boot_list[[h + 1]]    <- copy(placeholder)
    agg_list[[h + 1]]     <- copy(placeholder)
    agg_max_list[[h + 1]] <- copy(placeholder)
    hboot_list[[h + 1]]   <- copy(placeholder)
    next
  }
  
  # --- Create summary_dt for this hour ---
  summary_dt_h <- dt_hour[, .(prop_occupied = mean(EVSEStatus == "Occupied")), by = datetime]
  summary_dt_h[, hour := as.integer(format(datetime, "%H"))]
  
  # --- Apply your summary functions ---
  stats_list[[h + 1]]       <- summary_dt_h[, compute_stats(prop_occupied), by = hour] #data.table(compute_stats(summary_dt_b$prop_occupied), hour = 1)
  boot_list[[h + 1]]        <- bootstrapped_hourly_summary(summary_dt_h, n_boot = 1000) #data.table(bootstrapped_hourly_summary(summary_dt, n_boot = 1000), hour = h)
  agg_list[[h + 1]]         <- daily_hourly_summary(summary_dt_h) #data.table(daily_hourly_summary(summary_dt), hour = h)
  agg_max_list[[h + 1]]     <- daily_hourly_summary_max(summary_dt_h) #data.table(daily_hourly_summary_max(summary_dt), hour = h)
  hboot_list[[h + 1]]       <- hierarchical_hourly_bootstrap(summary_dt_h) #data.table(hierarchical_hourly_bootstrap(summary_dt), hour = h)
  
  # Clean up to save memory
  rm(dt_hour, summary_dt_h)
  gc()
}

# --- Combine hourly results into final 5 tables ---
hourly_summary       <- rbindlist(stats_list, use.names = TRUE, fill = TRUE)
hourly_summary_boot  <- rbindlist(boot_list, use.names = TRUE, fill = TRUE)
hourly_summary_agg   <- rbindlist(agg_list, use.names = TRUE, fill = TRUE)
hourly_summary_agg_max <- rbindlist(agg_max_list, use.names = TRUE, fill = TRUE)
hourly_summary_hboot <- rbindlist(hboot_list, use.names = TRUE, fill = TRUE)

cat("All hourly summaries completed.\n")
```


```{r visualization of cross-sectional data across the 5 methods}
utilization_visual(hourly_summary, title = "Hourly distibution of all utilization rates")
utilization_visual(hourly_summary, line = FALSE, title = "Hourly distibution of all utilization rates")
utilization_visual(hourly_summary_agg, title = "Hourly distribution of mean utilization rates")
utilization_visual(hourly_summary_agg, line = FALSE, title = "Hourly distribution of mean utilization rates")
utilization_visual(hourly_summary_agg_max, title = "Hourly distibution of maximum utilization rates")
utilization_visual(hourly_summary_agg_max, line = FALSE, title = "Hourly distibution of maximum utilization rates")
utilization_visual(hourly_summary_boot, title = "Hourly bootstrapped distribution of mean utilization rates")
utilization_visual(hourly_summary_boot, line = FALSE, title = "Hourly bootstrapped distribution of mean utilization rates")
utilization_visual(hourly_summary_hboot, title = "Hourly hierarchal bootstrapped distribution of mean utilization rates")
utilization_visual(hourly_summary_hboot, line = FALSE, title = "Hourly hierarchal bootstrapped distribution of mean utilization rates")
```



### Updated to here as of 15 October 2025







```{r function to read in csvs}
csv_to_df <- function(file) {
  result <- read.csv(file, as.is = "EvseID", stringsAsFactors = TRUE)
  
  # Find the position of "EvseID" column
  evse_col <- which(names(result) == "EvseID")

  # Keep columns starting from "EvseID"
  result <- result[, evse_col:ncol(result)]
  
  return(result)
}

```

```{r to read the "EVSE_partial" datasets into dfs - IF NEEDED}
EVSE_partial.1 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.1.csv")
EVSE_partial.2 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.2.csv")
EVSE_partial.3 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.3.csv")
EVSE_partial.4 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.4.csv")
EVSE_partial.5 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.5.csv")
EVSE_partial.6 <- csv_to_df("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Switzerland/Processed-data/EVSE_partial_step-1/EVSE_partial.6.csv")

```



#### Adding columns to processed charging dataset
```{r function to merge power level of each charger}
merge_power_location <- function(EVSE_partial_df) {
  EVSE_df <- merge(EVSE_partial_df, CH_EVSE_details_cleaned[ , c('EvseID','ChargingFacility_power','City')], by = 'EvseID', all.x = TRUE)

  #If any unmatched rows, attempt to match EvseID with ChargingStationId instead
  unmatched <- is.na(EVSE_df$ChargingFacility_power)
  
  if(any(unmatched)) {
    print("unmatched rows exist")
    unmatched_df <- EVSE_partial_df[unmatched, ]
  # Merge unmatched rows on ChargingStationId instead
  
    fallback_merge <- merge(unmatched_df, CH_EVSE_details_cleaned[ , c('ChargingStationId','ChargingFacility_power','City')],
      by.x = 'EvseID',
      by.y = 'ChargingStationId',
      all.x = TRUE
    )
   
    EVSE_df <- EVSE_df[!unmatched, ]
    EVSE_df <- rbind(EVSE_df, fallback_merge)

    print("merged df created")
  }
    
  EVSE_df <- EVSE_df %>%
    relocate(City, .after = EvseID) %>%
    relocate(ChargingFacility_power, .after = EvseID)
  
  return(EVSE_df)
}
```


```{r add charge point data (power and location) to the partial dfs}
EVSE_df.1 <- merge_power_location(EVSE_partial.1)
summary(EVSE_df.1$ChargingFacility_power)
EVSE_df.2 <- merge_power_location(EVSE_partial.2)
EVSE_df.3 <- merge_power_location(EVSE_partial.3)
EVSE_df.4 <- merge_power_location(EVSE_partial.4)
EVSE_df.5 <- merge_power_location(EVSE_partial.5)
EVSE_df.6 <- merge_power_location(EVSE_partial.6)
summary(EVSE_df.6$ChargingFacility_power)
```

```{r Check how many EvseIDs are not matching (we don't have data for them)}
length(EVSE_df.1$EvseID) #14350
length(EVSE_df.6$EvseID) #23485
length(CH_EVSE_details$EvseID) #19307
matches <- CH_EVSE_details$EvseID %in% EVSE_df.6$EvseID 
no_matches <- !matches
no_matches_EvseID <- CH_EVSE_details$EvseID[no_matches]

num_nomatches <- sum(no_matches)

EVSE_df.6_nomatch <- EVSE_df.6[no_matches,]

CH_EVSE_details_nomatch <- CH_EVSE_details[CH_EVSE_details$EvseID %in% no_matches_EvseID, ]

station_matches <- CH_EVSE_details_nomatch$ChargingStationId %in% no_matches_EvseID

sum(station_matches)

no_matches_stations <- CH_EVSE_details_nomatch[station_matches, ]


```

```{r Data snipit}
Full_EVSE_df <- EVSE_df.6 %>%
  select(EvseID,Power_kW, City)

unique(Full_EVSE_df$City)

write.csv(Full_EVSE_df, "M:/Rudolph/0-Datasets/Switzerland/Charging_point_IDs.csv")
```


## Utilization rate of charge poitns (EVSE IDs)
```{r function to get the utilization rate of each collected timestamp}
availability_summary <- function(df_input) { #takes the df object from function "read_csvs()" as the input
  # df_util <- data.frame(timestamp = NA, year = NA, month = NA, day = NA, weekday = NA, hour = NA, minute = NA, occupied = NA, available = NA, unknown = NA, service = NA, NA_ = NA, total = NA, utilization = NA)
  first <- TRUE
  for(col in colnames(df_input)) {
    if (!grepl("^S_", col)) next
    num_used <- sum(df_input[ ,col] == "Occupied", na.rm = TRUE) #number of occupied CPs
    num_available <- sum(df_input[ ,col] == "Available", na.rm = TRUE) #number of available CPs
    num_unknown <- sum(df_input[ ,col] == "Unknown", na.rm = TRUE) #number of unknown CPs
    num_service <- sum(df_input[ ,col] == "OutOfService", na.rm = TRUE) #number of out of service CPs
    num_NA <- sum(is.na(df_input[ ,col]), na.rm = TRUE) 
    tot_num <- length(df_input[ ,col])
    utilization_rate <- num_used/(num_used + num_available)
    
    #convert column name to a datetime object
    datetime_string <- sub("S_","",col)
    datetime_object <- ymd_hms(gsub("\\.", ":", datetime_string), tz = "Europe/Zurich")
    
    #record the data in a new results dataframe
    row_util <- data.frame(timestamp = datetime_object,
                           year = year(datetime_object), 
                           month = month(datetime_object),
                           day = day(datetime_object),
                           weekday = wday(datetime_object),
                           hour = hour(datetime_object),
                           minute = round(minute(datetime_object)/5)*5,
                           occupied = num_used, 
                           available = num_available, 
                           unknown = num_unknown, 
                           service = num_service,
                           NA_ = num_NA,
                           total = tot_num, 
                           utilization = utilization_rate)
    
    #make sure specific columns are integers
    int_cols <- c("year","month","day","weekday","hour","minute","occupied","available","unknown","service","NA_","total")
    row_util[ ,int_cols] <- lapply(row_util[ ,int_cols], as.integer)
    
    #bind with the large df
    if(first == TRUE) { #if this is the first row of the df
      df_util <- row_util
      first <- FALSE 
    } else {
      df_util <- rbind(df_util, row_util)
    }
    
  }
  return(df_util)
}

```


```{r SANDBOX}
EVSE_util.1 <- availability_summary(EVSE_df.1[ , 1:1000])

EVSE_util.1.winter <- EVSE_util.1 %>%
  filter(month %in% c(12,1,2))

summary(EVSE_util.1$month)
```

```{r segment by power level - NOT USED YET}
# summary(EVSE_df$Power_kW)
EVSE_lowP <- EVSE_df %>%
  filter(Power_kW <= 22)

EVSE_medP <- EVSE_df %>%
  filter(Power_kW <= 100 & Power_kW > 22)

EVSE_highP <- EVSE_df %>% 
  filter(Power_kW > 100)

EVSE_unknownP <- EVSE_df %>%
  filter(is.na(Power_kW))
```

```{r Utilization rate values - by power level - NOT USED YET}
#takes some time to run

EVSE_util_lowP <- Util_rate(EVSE_lowP)

# EVSE_util_medP <- Util_rate(EVSE_medP)

# EVSE_util_highP <- Util_rate(EVSE_highP)

EVSE_util_ovr <- Util_rate(EVSE_df)


```


### Utilization rate curves of each day type
```{r get utilization rate for all times in each day type}
# rm(list = c("win_wk_util","win_sa_util","win_su_util",
#             "spr_wk_util","spr_sa_util","spr_su_util",
#             "sum_wk_util","sum_sa_util","sum_su_util",
#             "fal_wk_util","fal_sa_util","fal_su_util"))

df_list <- c("EVSE_df.1","EVSE_df.2","EVSE_df.3","EVSE_df.4","EVSE_df.5","EVSE_df.6") #include more if there are other datasets included now

#NOTE: If you want to subset by something else (power level, location, etc) that must be done before this step.

for(df in 1:length(df_list)) {
  this_df <- get(df_list[df])
  this_df <- availability_summary(this_df) #get the utilization and time details from this partial dataset
  this_df$hour[this_df$hour == 0] <- 24 #as per conventions, hour 0 should be converted to hour 24
  #check
  leftover_rows <- nrow(this_df)
  
  #Note: Seasonal boundary months defined the same as in STEM-E
  #winter
  win_df <- this_df %>%
    filter(month %in% c(11,12,1))
  
  if(!exists("win_wk_util")) {
    win_wk_util <- filter(win_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(win_wk_util)
  } else {
    this_util <- filter(win_df, weekday %in% c(1,2,3,4,5))
    win_wk_util <- rbind(win_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("win_sa_util")) {
    win_sa_util <- filter(win_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(win_sa_util)
  } else {
    this_util <- filter(win_df, weekday == 6)
    win_sa_util <- rbind(win_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("win_su_util")) {
    win_su_util <- filter(win_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(win_su_util)
  } else {
    this_util <- filter(win_df, weekday == 7)
    win_su_util <- rbind(win_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  #spring
  spr_df <- this_df %>%
    filter(month %in% c(2,3,4))

  if(!exists("spr_wk_util")) {
    spr_wk_util <- filter(spr_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(spr_wk_util)
  } else {
    this_util <- filter(spr_df, weekday %in% c(1,2,3,4,5))
    spr_wk_util <- rbind(spr_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("spr_sa_util")) {
    spr_sa_util <- filter(spr_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(spr_sa_util)
  } else {
    this_util <- filter(spr_df, weekday == 6)
    spr_sa_util <- rbind(spr_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("spr_su_util")) {
    spr_su_util <- filter(spr_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(spr_su_util)
  } else {
    this_util <- filter(spr_df, weekday == 7)
    spr_su_util <- rbind(spr_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  #summer
  sum_df <- this_df %>%
    filter(month %in% c(5,6,7))
  
  if(!exists("sum_wk_util")) {
    sum_wk_util <- filter(sum_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(sum_wk_util)
  } else {
    this_util <- filter(sum_df, weekday %in% c(1,2,3,4,5))
    sum_wk_util <- rbind(sum_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("sum_sa_util")) {
    sum_sa_util <- filter(sum_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(sum_sa_util)
  } else {
    this_util <- filter(sum_df, weekday == 6)
    sum_sa_util <- rbind(sum_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("sum_su_util")) {
    sum_su_util <- filter(sum_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(sum_su_util)
  } else {
    this_util <- filter(sum_df, weekday == 7)
    sum_su_util <- rbind(sum_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  #fall
  fal_df <- this_df %>%
    filter(month %in% c(8,9,10))
  
  if(!exists("fal_wk_util")) {
    fal_wk_util <- filter(fal_df, weekday %in% c(1,2,3,4,5))
    leftover_rows <- leftover_rows - nrow(fal_wk_util)
  } else {
    this_util <- filter(fal_df, weekday %in% c(1,2,3,4,5))
    fal_wk_util <- rbind(fal_wk_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("fal_sa_util")) {
    fal_sa_util <- filter(fal_df, weekday == 6)
    leftover_rows <- leftover_rows - nrow(fal_sa_util)
  } else {
    this_util <- filter(fal_df, weekday == 6)
    fal_sa_util <- rbind(fal_sa_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  if(!exists("fal_su_util")) {
    fal_su_util <- filter(fal_df, weekday == 7)
    leftover_rows <- leftover_rows - nrow(fal_su_util)
  } else {
    this_util <- filter(fal_df, weekday == 7)
    fal_su_util <- rbind(fal_su_util,this_util)
    leftover_rows <- leftover_rows - nrow(this_util)
  }
  
  print(leftover_rows)
}

rm(list = c("spr_df","sum_df","fal_df","win_df")) #these were temporarily assigned in each loop


```


```{r}
get_timeslice_ci <- function(util_df, hrs = seq(1,24), z = 1.96) {
  max <- c()
  high <- c()
  median <- c()
  mean <- c()
  low <- c()
  min <- c()
  
  if(min(util_df$utilization, na.rm = TRUE) <= 0) warning("There are negative utilization rates in the dataframe.")
  if(sum(is.na(util_df$utilization)) > 0 ) warning("There are NA values for utilization rates in the dataframe.")
  
  for(hr in hrs) {
    filtered_df <- util_df %>%
      filter(hour == hr) 
  
    utilization <- filtered_df$utilization
    
    sample.mean <- mean(utilization, na.rm = TRUE)
    sample.median <- median(utilization, na.rm = TRUE)
    sample.sd <- sd(utilization, na.rm = TRUE)
    sample.n <- length(utilization)
    sample.min <- min(utilization, na.rm = TRUE)
    sample.max <- max(utilization, na.rm = TRUE)
    
    CI.high <- sample.mean + z*(sample.sd/sqrt(sample.n))
    CI.low <- sample.mean - z*(sample.sd/sqrt(sample.n))
    
    #collect values into vectors
    max <- c(max,sample.max)
    high <- c(high,CI.high)
    median <- c(median,sample.median)
    mean <- c(mean,sample.mean)
    low <- c(low,CI.low)
    min <- c(min,sample.min)
    
  }
  
  #return the dataframe of the vectors collected
  return(data.frame("hour" = hrs,
                    "max" = max,
                    "CI_high" = high,
                    "median" = median,
                    "mean" = mean,
                    "CI_low" = low,
                    "min" = min))
  
}
```


```{r Get overall utilization curves}
daytype_names <- c("win_wk","win_sa","win_su",
                   "spr_wk","spr_sa","spr_su",
                   "sum_wk","sum_sa","sum_su",
                   "fal_wk","fal_sa","fal_su")

# day <- daytype_names[2]

for(day in daytype_names) {
  CI_df_name <- paste0(day,"_CI")
  df_name <- paste0(day,"_util")
  
  this_df <- get(df_name)
  
  this_CI_df <- get_timeslice_ci(this_df, z = 2.576)
  
  assign(CI_df_name, this_CI_df)
}

#Result of this block is to have several day type dataframes with the confidence interval, minimum, and maximums over each hour (and therefore, each timeslice)
```

```{r Save full utilization dataframes}
daytype_folder <- "M:/Rudolph/R-Projects/swiss-public-charging/data/daytype_dataframes"
for(day in daytype_names) {
  df_name <- paste0(day,"_util")
  csv_name <- paste0(df_name,".csv")
  
  this_df <- get(df_name)
  
  filepath <- file.path(daytype_folder, csv_name)  
  write.csv(this_df, filepath, row.names = FALSE)
  
}
```


```{r Save confidence interval dataframes}
daytype_ci_folder <- "M:/Rudolph/R-Projects/swiss-public-charging/data/daytype_ci"
for(day in daytype_names) {
  CI_df_name <- paste0(day,"_CI")
  csv_name <- paste0(CI_df_name,".csv")
  
  this_df <- get(CI_df_name)
  
  filepath <- file.path(daytype_ci_folder, csv_name)  
  write.csv(this_df, filepath, row.names = FALSE)
}
  
```

```{r Create and save a df that can be copy/pasted into VEDA}
#needs a column for timeslice in the form SPR-WK-D01 and the utilization rate value (as a decimal)
hrs <- seq(1,24)
TimeSlice <- c()
Utilization_min <- c()
Utilization_median <- c()
Utilization_max <- c()

day <- daytype_names[1]
for(day in daytype_names) {
  this_df_name <- paste0(day,"_CI")
  this_df <- get(this_df_name)
  for(hr in hrs) {
    this_TS_name <- sprintf("%s-D%02d", gsub("_", "-", toupper(day)), hr) #correctly formats the string to fit VEDA needs
    this_utilization_min <- this_df[this_df$hour == hr, "min"]
    this_utilization_median <- this_df[this_df$hour == hr, "median"]
    this_utilization_max <- this_df[this_df$hour == hr, "max"]
    
    TimeSlice <- c(TimeSlice,this_TS_name)
    Utilization_min <- c(Utilization_min,this_utilization_min)
    Utilization_median <- c(Utilization_median,this_utilization_median)
    Utilization_max <- c(Utilization_max,this_utilization_max)

  }
}

result <- data.frame("TimeSlice" = TimeSlice,
                     "Utilization_min" = Utilization_min,
                     "Utilization_median" = Utilization_median,
                     "Utilization_max" = Utilization_max)



```

```{r}
write.csv(result,  "M:/Rudolph/R-Projects/swiss-public-charging/data/timeslice_ci.csv")
```


#Visualizations of utlization
```{r Visualization function}
visualize_utilization_summary <- function(util_CI_df, title = "Utilization Summary") {
  y_max <- ceiling(max(util_CI_df$max)*100)
  y_max <- y_max + 5 - (y_max %% 5) #gets the maximum value up to the nearest 5
  
  plot(x = util_CI_df$hour, y = util_CI_df$median*100, 
     ylim = c(0,y_max), ylab = "Utilization [%]", xlab = "Hour", 
     main = title,
     col = 'black', cex = 0.8, type = 'l')
  lines(x = util_CI_df$hour, y = util_CI_df$CI_low*100, col = 'blue')
  lines(x = util_CI_df$hour, y = util_CI_df$CI_high*100, col = 'blue')
  lines(x = util_CI_df$hour, y = util_CI_df$min*100, col = 'red')
  lines(x = util_CI_df$hour, y = util_CI_df$max*100, col = 'red')

}

```

```{r}
visualize_utilization_summary(win_wk_CI)
```

NOTE: 
this confidence interval is super tight which indicates that there could be some autocorrelation between the hours, plus the fact that I have 20 observations per hour (every 5 minutes. I might want to consider a bootstrap method to get the confidence interval in a future iteration.)
```{r Bootstrap code}

bootstrap_ci <- function(data, num_bootstrap_samples = 10000, ci = 95) {
  boot_samples <- replicate(num_bootstrap_samples, median(sample(data, replace = TRUE)))
  
  lower <- quantile(boot_samples, (100 - ci) / 2 / 100)
  upper <- quantile(boot_samples, 1 - (100 - ci) / 2 / 100)
  
  return(c(lower, upper))
}

# Compute bootstrap confidence intervals for each hour
hourly_cis <- df %>%
  group_by(hour) %>%
  summarise(
    Lower_CI = bootstrap_ci(utilization_rate)[1],
    Upper_CI = bootstrap_ci(utilization_rate)[2]
  )

# Print the results
print(hourly_cis)

```







```{r function to get certain parameters}
Subset_EVSE <- function(df_input, year.f = NULL, month.f = NULL, day.f = NULL, weekday.f = NULL, hour.f = NULL) {
  df_result <- df_input
  
  if(!is.null(year.f)) {df_result <- filter(df_result, year == year.f)}
  if(!is.null(month.f)) {df_result <- filter(df_result, month == month.f)}
  if(!is.null(day.f)) {df_result <- filter(df_result, day == day.f)}
  if(!is.null(weekday.f)) {df_result <- filter(df_result, weekday == weekday.f)}
  if(!is.null(hour.f)) {df_result <- filter(df_result, hour == hour.f)}
  
  return(df_result)
}

Get_CI <- function(df_input, col_name = "utilization", z_score = 1.96, clean.na = TRUE) {
  observations <- df_input[col_name]
  
  if(clean.na == TRUE)  observations <- observations[!is.na(observations)]
  
  sample_mean <- mean(observations)
  sample_sd <- sd(observations)
  sample_size <- length(observations)
  sample_min <- min(observations)
  sample_max <- max(observations)
  
  sample_median <- median(observations)
  
  std_err <- sample_sd/ sqrt(sample_size)
  
  lower_bound <- sample_mean - (z_score * std_err)
  upper_bound <- sample_mean + (z_score * std_err)
  
  
  return(list("lower_bound" = lower_bound,
              "mean" = sample_mean,
              "upper_bound" = upper_bound,
              "min" = sample_min,
              "max" = sample_max))
}
```

```{r sandbox}
filter(EVSE_util_wkday, !is.na("utilization"))

undebug(Get_CI)
Weekday.CI <- Get_CI(EVSE_util_wkday)
```


```{r weedays}
EVSE_util_wkday <- EVSE_util_lowP %>%
  filter(weekday <= 5)


#get the confidence interval for each hour
for(h in 0:23) {
  this.df <- Subset_EVSE(EVSE_util_wkday, hour.f = h)
  
  this.CI <- Get_CI(this.df)
  
  result.df <- data.frame(hour = h, 
                          lower_bound = this.CI$lower_bound,
                          mean = this.CI$mean,
                          upper_bound = this.CI$upper_bound,
                          min = this.CI$min,
                          max = this.CI$max)
  
  if(h == 0) {
    result <- result.df
  } else {
    result <- rbind(result, result.df)
    }
}

#visualizations
result

plot(x = EVSE_util_wkday$hour, y = EVSE_util_wkday$utilization*100, 
     ylim = c(0,20), ylab = "Utilization [%]", xlab = "Hour", 
     col = 'black', pch = 19, cex = 0.8)

plot(result$hour, result$mean*100, type = 'l', col = 'black', 
     xlab = "hour", ylab = "utilization [%]", ylim = c(0,20),
     main = "Weekdays")
lines(result$hour, result$lower_bound*100, type = 'l', col = 'blue')
lines(result$hour, result$upper_bound*100, type = 'l', col = 'blue')
lines(result$hour, result$min*100, type = 'l', col = 'red')
lines(result$hour, result$max*100, type = 'l', col = 'red')

```
```{r}
hist(filter(result, hour = 10))
```

```{r clean data}
EVSE_util_df_cleaned <- EVSE_util_df %>%
  filter(unknown < dim(EVSE_util_df)[1] * 0.1) %>% #remove instances where 'unknown' CPs are too high, this is likely a data collection error
  merge(Charger_df, Status_df, by = "EvseID", all = TRUE)

```

```{r plot utilization}
plot(x = as.POSIXlt(EVSE_util_df_cleaned$timestamp), y = EVSE_util_df_cleaned$utilization, pch = 19,
     ylab = "Utilization Rate", xlab = "Date", ylim = c(0,0.25))

```


```{r filter for observations}

status_filter <- function(data, )

```

```{r EVSE details}

Power_demand <- function(Charger_df, Status_df) {
  Charger_df <- Charger_df %>%
    select(EvseID, Power_kW)
  
  # Add the power demand column to the status data frame
  Power_status_df <- merge(Charger_df, Status_df, by = "EvseID", all = TRUE)
  
  # convert "occupied" into the power level for all the status columns
 for(col in 3:dim(Power_status_df)[2]) {
   Power_status_df[ , col] <- Power_status_df[ , "Power_kW"] * as.numeric(Power_status_df[ , col] == "Occupied")
 }
  return(Power_status_df) 
}

```

```{r convert statuses into power level demands}
EVSE_df <- Power_demand(CH_EVSE_details_short, EVSE_partial.B)
```


```{r clean power status}
Power_status_df_cleaned <- Power_status_df %>%
  filter(!is.na(Power_kW))
```


```{r power level}
Power_level <- function(df_input, filters = NULL) { #takes the df object from function "Power_demand()" as the input
  df_power <- data.frame(timestamp = NA, year = NA, month = NA, day = NA, weekday = NA, hour = NA, minute = NA, power = NA)
  for(col in colnames(df_input[-c(1:2)])) {
    power_dem <- sum(df_input[ , col], na.rm = TRUE) #gets the total power level
    
    #convert column name to a datetime object
    datetime_string <- sub("S_","",col)
    datetime_object <- ymd_hms(gsub("\\.", ":", datetime_string), tz = "Europe/Zurich")
    
    #record the data in a new results dataframe
    row_pwr <- data.frame(timestamp = datetime_object,
                           year = year(datetime_object), 
                           month = month(datetime_object),
                           day = day(datetime_object),
                           weekday = wday(datetime_object),
                           hour = hour(datetime_object),
                           minute = round(minute(datetime_object)/5)*5,
                           power = power_dem)
    
    #bind with the large df
    df_power <- rbind(df_power, row_pwr)
  }
  return(df_power)
}

```

```{r power level data}
#takes time to run
Power_results <- Power_level(Power_status_df_cleaned)
```
```{r power level data cleaned}
Power_results_cleaned <- Power_results %>%
  filter(power > 0)

```


```{r plot power}
plot(x = as.POSIXlt(Power_results_cleaned$timestamp), y = Power_results_cleaned$power/1000, pch = 19,
     ylab = "Power Demand [MW]", xlab = "Date", ylim = c(0,20))
```

