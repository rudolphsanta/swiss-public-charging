---
title: "Raw Data Processing"
author: "R Santarromana"
date: "2025-09-04"
output: html_document
---

#This file converts collected raw data from public charging (.json files) into .csv files which can then be read into data tables.
#The result will be to convert raw jsons into .csv files saved in a chosen directory that can be loaded later.

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(jsonlite)
library(tidyr)
library(stringr)
library(lubridate)
```

```{r file locations}
#Locations of the main folders for input and output.

##All the raw data are collected and stored here
raw_data_root <- file.path("P:/Rudolph/Swiss-EVChg-Data") #there are monthly folders in this directory

output_root <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_CSV")

```

```{r functions for processing}
safe_fromJSON <- function(path) {
  tryCatch({
    jsonlite::fromJSON(path)
  }, error = function(e) {
    warning("Failed to parse file: ", path, " â€” ", conditionMessage(e))
    return(NULL)  # or return an empty data frame
  })
}


json_to_df <- function(json_file_path) {
  json_data <- safe_fromJSON(json_file_path)
  if (is.null(json_data)) return(NULL)   # skip bad file
  
  # raw timestamp
  timestamp_raw <- basename(json_file_path) %>%
    str_remove("data-") %>%
    str_remove(".json") %>%
    as.POSIXct(format = "%Y-%m-%d-%H-%M-%S", tz = "Europe/Zurich")
  
  timestamp_rounded <- lubridate::round_date(timestamp_raw, unit = "5 minutes")
  
  num_operators <- length(json_data$EVSEStatuses$EVSEStatusRecord)

  #initialize empty dataframe
  all_op_statuses_df <- data.frame()
  
  #all other operators
  for(n in 1:num_operators) {
    this_op_statuses_df <- json_data$EVSEStatuses$EVSEStatusRecord[[n]]
    # Skip if no records
    if (is.null(this_op_statuses_df) || nrow(this_op_statuses_df) == 0) next
    
    this_op_statuses_df$OperatorID <- json_data$EVSEStatuses$OperatorID[[n]]
    this_op_statuses_df$OperatorName <- json_data$EVSEStatuses$OperatorName[[n]]
    
    all_op_statuses_df <- rbind(all_op_statuses_df, this_op_statuses_df)
  }

  all_op_statuses_df <- all_op_statuses_df %>%
    transmute(datetime = timestamp_rounded, #force the column into the expected timestamp
              # full_datetime = format(timestamp, "%Y-%m-%d-%H-%M"),  # for viewing/export
              EvseID = as.factor(EvseID),
              EVSEStatus = as.factor(EVSEStatus),
              OperatorID = as.factor(OperatorID),
              OperatorName = as.factor(OperatorName)) %>%
    select(datetime, everything())
  
  return(all_op_statuses_df)
}


operator_dict <- function(json_file_path, existing_operator_dict = NULL) {
  json_data <- fromJSON(json_file_path)
  num_operators <- length(json_data$EVSEStatuses)

  op_dict <- data.frame("OperatorID"= NULL, "OperatorName" = NULL)
  
  for(n in 1:num_operators) {
    op_ID <- json_data$EVSEStatuses[[n]][[2]]
    op_name <- json_data$EVSEStatuses[[n]][[3]]
    this_op <- data.frame("OperatorID" = op_ID, "OperatorName" = op_name)
    
    op_dict <- rbind(op_dict, this_op)
  }
  
  if(!is.null(existing_operator_dict)){
    op_dict <- merge(existing_operator_dict, by = "OperatorName", all.x = TRUE)
  }
  return(op_dict)
}
```


```{r function for sequential execution}
raw_data_merge <- function(root_folder, sub_folder, output_file_path = NULL) {
  start <- Sys.time()
  full_path <- file.path(root_folder,sub_folder)
  
  month_status_files <- list.files(full_path, full.names = TRUE)
  #filter only those files with minutes ending in 0 or 5
  month_status_files <- month_status_files[grepl("-[0-5][05]-\\d{2}\\.json$", month_status_files)]
  
  month_df_raw <- bind_rows(lapply(month_status_files, json_to_df))
  
  if(!is.null(output_file_path)) {
    # Save the merged data to a CSV file
    write.csv(month_df_raw, file = output_file_path, row.names = FALSE)
  } 
  
  end <- Sys.time()
  duration <- round(difftime(end, start, units = "mins"), 2)
  cat("Total Exeuction Time for Sequential Run: ", duration, "minutes.\n")
  
  return(month_df_raw)
}

```


```{r function for parallel execution}
library(parallel)

raw_data_merge_parallel <- function(root_folder, sub_folder, output_file_path = NULL) {
  start <- Sys.time()
  full_path <- file.path(root_folder,sub_folder)

  month_status_files <- list.files(full_path, full.names = TRUE, pattern = "\\.json$")
  
  timestamps <- str_extract(month_status_files, "\\d{4}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}")
  datetimes <- as.POSIXct(timestamps, format = "%Y-%m-%d-%H-%M-%S", tz = "Europe/Zurich")
  
  month_start <- floor_date(min(datetimes), "month")
  month_end <- ceiling_date(max(datetimes), "month") - minutes(5)
  expected_times <- seq(from = month_start, to = month_end, by = "5 min")
  
  find_nearest <- function(target, available) {
    diffs <- abs(difftime(available, target, units = "secs"))
    available[which.min(diffs)]
  }
  
  matched_times <- sapply(expected_times, find_nearest, available = datetimes)
  diff_secs <- as.numeric(difftime(matched_times, expected_times, units = "secs"))
  
  # Only show problematic (non-exact) intervals
  diagnostic_log <- data.frame(
    expected_time = expected_times,
    matched_time = matched_times,
    time_diff_secs = diff_secs,
    matched_file = month_status_files[match(matched_times, datetimes)],
    stringsAsFactors = FALSE
  ) %>%
    filter(abs(time_diff_secs) > 2) %>%
    arrange(expected_time)
  
  if (any(abs(diff_secs) > 150)) {
    warning("Some expected 5-min intervals have no close match (>2.5 min difference).")
  }
  
  corrected_files <- month_status_files[match(matched_times, datetimes)]
  override_times <- expected_times
  
  ##parallel setup
  num_cores <- detectCores() - 2 #leave some cores free
  cl <- makeCluster(num_cores)
  
  clusterExport(cl, varlist = c("json_to_df","safe_fromJSON"))
  clusterEvalQ(cl, {library(jsonlite); library(dplyr); library(stringr); library(lubridate)})
  
  #parallel execution
  month_df_list <- parLapply(cl, month_status_files, json_to_df)
  
  # parallel map with both file path and override timestamp
  # month_df_list <- parLapply(
  #   cl,
  #   seq_along(corrected_files),
  #   function(i) json_to_df(corrected_files[i], override_time = override_times[i])
  # )
  
  stopCluster(cl)
  
  #merge results
  month_df_raw <- bind_rows(month_df_list)
  
  if(!is.null(output_file_path)) {
    # Save the merged data to a CSV file
    write.csv(month_df_raw, file = output_file_path, row.names = FALSE)
  } 
    
  end <- Sys.time()
  duration <- round(difftime(end, start, units = "mins"), 2)
  cat("Total Exeuction Time for Parallel Run: ", duration, "minutes.\n")
  return(list(
    data = month_df_raw,
    diagnostics = diagnostic_log
  ))
}

```

```{r count how many .jsons are merged only}

count_5min_snapshots <- function(root_folder, sub_folder, print = TRUE) {
  start <- Sys.time()
  full_path <- file.path(root_folder, sub_folder)
  
  # list JSON files
  month_status_files <- list.files(full_path, full.names = TRUE, pattern = "\\.json$")
  
  # extract timestamps
  timestamps <- str_extract(month_status_files, "\\d{4}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}")
  datetimes <- as.POSIXct(timestamps, format = "%Y-%m-%d-%H-%M-%S", tz = "Europe/Zurich")
  
  # remove invalid timestamps
  datetimes <- datetimes[!is.na(datetimes)]
  
  if (length(datetimes) == 0) {
    warning("No valid timestamps found in file names.")
    return(NULL)
  }
  
  # round to nearest 5-min block
  datetimes_5min <- round_date(datetimes, "5 minutes")
  
  # determine the month boundaries
  month_start <- floor_date(min(datetimes_5min), "month")
  month_end <- ceiling_date(max(datetimes_5min), "month") - minutes(5)
  expected_times <- seq(from = month_start, to = month_end, by = "5 min")
  
  # count unique and expected intervals
  unique_times <- unique(datetimes_5min)
  matched <- expected_times[expected_times %in% unique_times]
  missing <- setdiff(expected_times, unique_times)
  
  num_unique_snapshots <- length(unique_times)
  num_expected_snapshots <- length(expected_times)
  
  # theoretical full-month 5-min count (from 1st to last day of the month)
  year <- as.integer(str_sub(sub_folder, 1, 4))
  month <- as.integer(str_sub(sub_folder, 6, 7))
  theoretical_start <- ymd_hms(sprintf("%04d-%02d-01 00:00:00", year, month), tz = "Europe/Zurich")
  theoretical_end <- theoretical_start + months(1) - minutes(5)
  # theoretical_end <- ceiling_date(theoretical_start, "month") - minutes(5)
  theoretical_times <- seq(from = theoretical_start, to = theoretical_end, by = "5 min")
  num_theoretical_snapshots <- length(theoretical_times)
  
  pct_coverage <- round(100 * num_unique_snapshots / num_theoretical_snapshots, 2)
  
  end <- Sys.time()
  duration <- round(difftime(end, start, units = "secs"), 2)
  
  if(print == TRUE) {
    cat("---- 5-Min Snapshot Summary ----\n")
    cat("Folder: ", sub_folder, "\n")
    cat("Total JSON files found: ", length(month_status_files), "\n")
    cat("Unique 5-min intervals observed: ", num_unique_snapshots, "\n")
    cat("Observed range: ", format(month_start, "%Y-%m-%d %H:%M"), "to ", format(month_end, "%Y-%m-%d %H:%M"), "\n")
    cat("Expected intervals (based on timestamps): ", num_expected_snapshots, "\n")
    cat("Theoretical range: ", format(theoretical_start, "%Y-%m-%d %H:%M"), "to ", format(theoretical_end, "%Y-%m-%d %H:%M"), "\n")
    cat("Theoretical full-month intervals: ", num_theoretical_snapshots, "\n")
    cat("Coverage: ", pct_coverage, "%\n")
    cat("Execution time: ", duration, " sec\n")
  }
  
  
  return(list(
    folder = sub_folder,
    total_files = length(month_status_files),
    unique_snapshots = num_unique_snapshots,
    expected_snapshots = num_expected_snapshots,
    theoretical_snapshots = num_theoretical_snapshots,
    coverage_percent = pct_coverage,
    missing_intervals = missing
  ))
}
```


```{r count total and expected snapshots}
#single month count
count <- count_5min_snapshots(raw_data_root, "2024-10")

#total dataset counts
first_month <- "2022-12"
last_month <- "2025-10"

ovr_seq_start <- as.POSIXct(paste0(first_month, "-01 00:00"), format="%Y-%m-%d %H:%M", tz = "Europe/Zurich") - minutes(5) + minutes(5)
ovr_seq_end   <- as.POSIXct(paste0(last_month, "-01 00:00"), format="%Y-%m-%d %H:%M", tz = "Europe/Zurich") + months(1) - minutes(5)

# Generate 5-minute sequence
ovr_seq <- seq(from = ovr_seq_start, to = ovr_seq_end, by = "5 min")
length(ovr_seq)  # total number of theoretical 5-min intervals

# --- Step 2: Generate vector of months between first and last month inclusive ---
# Convert first/last month to Date objects (first day of month)
first_month_date <- as.Date(paste0(first_month, "-01"))
last_month_date  <- as.Date(paste0(last_month, "-01"))

# Sequence of months
month_seq <- seq(from = first_month_date, to = last_month_date, by = "month")
month_seq_fmt <- format(month_seq, "%Y-%m")  # "YYYY-MM" format

# Count actual snapshots over the horizon
count_ovr <- 0
for(month in month_seq_fmt) {
  result <- tryCatch({
    count_5min_snapshots(raw_data_root, month, print = FALSE)
  }, warning = function(w) {
    message("Warning skipped for month: ", month, " - ", conditionMessage(w))
    return(NULL)
  }, error = function(e) {
    message("Error skipped for month: ", month, " - ", conditionMessage(e))
    return(NULL)
  })
  
  if(is.null(result)) next
  
  count_ovr <- count_ovr + result$unique_snapshots
}

cat("Total snapshots observed: ", count_ovr, "\n")
cat("Total theoretical snapshots: ", length(ovr_seq), "\n")
cat("Date Range: ", format(ovr_seq_start, "%Y-%m-%d %H:%M"), "to ", format(ovr_seq_end, "%Y-%m-%d %H:%M"), "\n")
cat("Percent observed", (count_ovr/length(ovr_seq))*100, "%\n")
```

```{r SANDBOX}
sub_folder <- "2025-09"
full_path <- file.path(raw_data_root, sub_folder)

# Get list of all files
month_status_files <- list.files(full_path, full.names = TRUE)

# Optional: filter only files with minutes ending in 0 or 5
month_status_files <- month_status_files[grepl("-[0-5][05]-\\d{2}\\.json$", month_status_files)]

month_status_files[1]

debug(json_to_df)
dec_json <- json_to_df(month_status_files[1])
```

#This block can be run every month when all the jsons from the prior month are collected. No changes needed to the block, it automatically checks today's date and collects the block of the last full month IF it doesn't exist yet.

```{r loop for creating csvs}
first_month <- "2022-12"
last_month <- format(seq(Sys.Date(), length = 2, by = "-1 month")[2], "%Y-%m") #automatically detects which month it is and gets the last full month.

# Convert to Date objects (assume first of the month)
first_date <- as.Date(paste0(first_month, "-01"))
last_date  <- as.Date(paste0(last_month, "-01"))

# Generate sequence by months
month_seq <- seq(first_date, last_date, by = "month")

# Convert back to "YYYY-MM"
month_vec <- format(month_seq, "%Y-%m")

for(month in month_vec) {
  #generate the full filepath for the input and output.
  prefix <- paste0(month,"_charging")
  output_name <- paste0(prefix, ".csv")
  output_path <- file.path(output_root,output_name)
  
  # Get all files in the output directory
  existing_files <- list.files(output_root, full.names = FALSE)

  #check if the file exists
  if(any(startsWith(existing_files, prefix))) {
    cat("Skipping", month, "- found existing file(s):",
        paste(existing_files[startsWith(existing_files, prefix)], collapse = ", "), "\n")
    next

  } 
    
  raw_data_merge_parallel(raw_data_root,month,output_path)
 
}


```


#.csv files now saved to the chosen output directory.

```{r csv conversion to Parquet - If needed}
library(arrow)
library(tools)
library(data.table)

csv_to_parquet <- function(csv_file_paths, dest_folder) {
  # Create destination folder if it doesn't exist
  if (!dir.exists(dest_folder)) {
    dir.create(dest_folder, recursive = TRUE)
  }
  
  for (csv_path in csv_file_paths) {
    # Only proceed if file truly ends with .csv (not .csv.zip etc.)
    if (!grepl("\\.csv$", csv_path, ignore.case = TRUE)) {
      next
    }

    # Build output Parquet path
    file_base <- file_path_sans_ext(basename(csv_path))  # strip .csv
    parquet_path <- file.path(dest_folder, paste0(file_base, ".parquet"))
    
    # Skip if Parquet file already exists
    if (file.exists(parquet_path)) {
      message("Skipping (already exists): ", parquet_path)
      next
    }
    
    # Read CSV robustly, filling missing columns with NA
    dt <- fread(csv_path, fill = TRUE)
    
    # Write Parquet
    arrow::write_parquet(dt, parquet_path)
    
    message("Converted: ", csv_path, " -> ", parquet_path)
  }
}

```

```{r}
charging_data_root <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_CSV")
chg_avail_csvs <- list.files(charging_data_root,full.names = TRUE)

output_parquet_path <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_Parquet")


csv_to_parquet(chg_avail_csvs, output_parquet_path)
```

#.parquet files now saved to the chosen output directory.