---
title: "Raw Data Processing"
author: "R Santarromana"
date: "2025-09-04"
output: html_document
---

#This file converts collected raw data from public charging (.json files) into .csv files which can then be read into data tables.
#The result will be to convert raw jsons into .csv files saved in a chosen directory that can be loaded later.

```{r setup, include=FALSE}
library(dplyr)
library(tidyverse)
library(jsonlite)
library(tidyr)
library(stringr)
library(lubridate)
```

```{r file locations}
#Locations of the main folders for input and output.

##All the raw data are collected and stored here
raw_data_root <- file.path("P:/Rudolph/Swiss-EVChg-Data") #there are monthly folders in this directory

output_root <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_CSV")

```

```{r functions for processing}
safe_fromJSON <- function(path) {
  tryCatch({
    jsonlite::fromJSON(path)
  }, error = function(e) {
    warning("Failed to parse file: ", path, " â€” ", conditionMessage(e))
    return(NULL)  # or return an empty data frame
  })
}


json_to_df <- function(json_file_path) {
  json_data <- safe_fromJSON(json_file_path)
  if (is.null(json_data)) return(NULL)   # skip bad file
  
  # raw timestamp
  timestamp_raw <- basename(json_file_path) %>%
    str_remove("data-") %>%
    str_remove(".json") %>%
    as.POSIXct(format = "%Y-%m-%d-%H-%M-%S", tz = "Europe/Zurich")
  
  timestamp_rounded <- lubridate::round_date(timestamp_raw, unit = "5 minutes")
  
  num_operators <- length(json_data$EVSEStatuses$EVSEStatusRecord)

  #initialize empty dataframe
  all_op_statuses_df <- data.frame()
  
  #all other operators
  for(n in 1:num_operators) {
    this_op_statuses_df <- json_data$EVSEStatuses$EVSEStatusRecord[[n]]
    # Skip if no records
    if (is.null(this_op_statuses_df) || nrow(this_op_statuses_df) == 0) next
    
    this_op_statuses_df$OperatorID <- json_data$EVSEStatuses$OperatorID[[n]]
    this_op_statuses_df$OperatorName <- json_data$EVSEStatuses$OperatorName[[n]]
    
    all_op_statuses_df <- rbind(all_op_statuses_df, this_op_statuses_df)
  }

  all_op_statuses_df <- all_op_statuses_df %>%
    transmute(datetime = timestamp_rounded, #force the column into the expected timestamp
              # full_datetime = format(timestamp, "%Y-%m-%d-%H-%M"),  # for viewing/export
              EvseID = as.factor(EvseID),
              EVSEStatus = as.factor(EVSEStatus),
              OperatorID = as.factor(OperatorID),
              OperatorName = as.factor(OperatorName)) %>%
    select(datetime, everything())
  
  return(all_op_statuses_df)
}


operator_dict <- function(json_file_path, existing_operator_dict = NULL) {
  json_data <- fromJSON(json_file_path)
  num_operators <- length(json_data$EVSEStatuses)

  op_dict <- data.frame("OperatorID"= NULL, "OperatorName" = NULL)
  
  for(n in 1:num_operators) {
    op_ID <- json_data$EVSEStatuses[[n]][[2]]
    op_name <- json_data$EVSEStatuses[[n]][[3]]
    this_op <- data.frame("OperatorID" = op_ID, "OperatorName" = op_name)
    
    op_dict <- rbind(op_dict, this_op)
  }
  
  if(!is.null(existing_operator_dict)){
    op_dict <- merge(existing_operator_dict, by = "OperatorName", all.x = TRUE)
  }
  return(op_dict)
}
```


```{r function for sequential execution}
raw_data_merge <- function(root_folder, sub_folder, output_file_path = NULL) {
  start <- Sys.time()
  full_path <- file.path(root_folder,sub_folder)
  
  month_status_files <- list.files(full_path, full.names = TRUE)
  #filter only those files with minutes ending in 0 or 5
  month_status_files <- month_status_files[grepl("-[0-5][05]-\\d{2}\\.json$", month_status_files)]
  
  month_df_raw <- bind_rows(lapply(month_status_files, json_to_df))
  
  if(!is.null(output_file_path)) {
    # Save the merged data to a CSV file
    write.csv(month_df_raw, file = output_file_path, row.names = FALSE)
  } 
  
  end <- Sys.time()
  duration <- round(difftime(end, start, units = "mins"), 2)
  cat("Total Exeuction Time for Sequential Run: ", duration, "minutes.\n")
  
  return(month_df_raw)
}

```


```{r function for parallel execution}
library(parallel)

raw_data_merge_parallel <- function(root_folder, sub_folder, output_file_path = NULL) {
  start <- Sys.time()
  full_path <- file.path(root_folder,sub_folder)

  month_status_files <- list.files(full_path, full.names = TRUE, pattern = "\\.json$")
  
  timestamps <- str_extract(month_status_files, "\\d{4}-\\d{2}-\\d{2}-\\d{2}-\\d{2}-\\d{2}")
  datetimes <- as.POSIXct(timestamps, format = "%Y-%m-%d-%H-%M-%S", tz = "Europe/Zurich")
  
  month_start <- floor_date(min(datetimes), "month")
  month_end <- ceiling_date(max(datetimes), "month") - minutes(5)
  expected_times <- seq(from = month_start, to = month_end, by = "5 min")
  
  find_nearest <- function(target, available) {
    diffs <- abs(difftime(available, target, units = "secs"))
    available[which.min(diffs)]
  }
  
  matched_times <- sapply(expected_times, find_nearest, available = datetimes)
  diff_secs <- as.numeric(difftime(matched_times, expected_times, units = "secs"))
  
  # Only show problematic (non-exact) intervals
  diagnostic_log <- data.frame(
    expected_time = expected_times,
    matched_time = matched_times,
    time_diff_secs = diff_secs,
    matched_file = month_status_files[match(matched_times, datetimes)],
    stringsAsFactors = FALSE
  ) %>%
    filter(abs(time_diff_secs) > 2) %>%
    arrange(expected_time)
  
  if (any(abs(diff_secs) > 150)) {
    warning("Some expected 5-min intervals have no close match (>2.5 min difference).")
  }
  
  corrected_files <- month_status_files[match(matched_times, datetimes)]
  override_times <- expected_times
  
  ##parallel setup
  num_cores <- detectCores() - 2 #leave some cores free
  cl <- makeCluster(num_cores)
  
  clusterExport(cl, varlist = c("json_to_df","safe_fromJSON"))
  clusterEvalQ(cl, {library(jsonlite); library(dplyr); library(stringr); library(lubridate)})
  
  #parallel execution
  month_df_list <- parLapply(cl, month_status_files, json_to_df)
  
  # parallel map with both file path and override timestamp
  # month_df_list <- parLapply(
  #   cl,
  #   seq_along(corrected_files),
  #   function(i) json_to_df(corrected_files[i], override_time = override_times[i])
  # )
  
  stopCluster(cl)
  
  #merge results
  month_df_raw <- bind_rows(month_df_list)
  
  if(!is.null(output_file_path)) {
    # Save the merged data to a CSV file
    write.csv(month_df_raw, file = output_file_path, row.names = FALSE)
  } 
    
  end <- Sys.time()
  duration <- round(difftime(end, start, units = "mins"), 2)
  cat("Total Exeuction Time for Parallel Run: ", duration, "minutes.\n")
  return(list(
    data = month_df_raw,
    diagnostics = diagnostic_log
  ))
}

```

```{r execution trials}
mar_24_seq <- raw_data_merge(raw_data_root, "2024-03")

undebug(raw_data_merge_parallel)
sep_25_par <- raw_data_merge_parallel(raw_data_root, "2025-09")

summary(sep_25_par$diagnostics)

```

```{r SANDBOX}
sub_folder <- "2025-09"
full_path <- file.path(raw_data_root, sub_folder)

# Get list of all files
month_status_files <- list.files(full_path, full.names = TRUE)

# Optional: filter only files with minutes ending in 0 or 5
month_status_files <- month_status_files[grepl("-[0-5][05]-\\d{2}\\.json$", month_status_files)]

month_status_files[1]

debug(json_to_df)
dec_json <- json_to_df(month_status_files[1])
```

#This block can be run every month when all the jsons from the prior month are collected. No changes needed to the block, it automatically checks today's date and collects the block of the last full month IF it doesn't exist yet.

```{r loop for creating csvs}
first_month <- "2022-12"
last_month <- format(seq(Sys.Date(), length = 2, by = "-1 month")[2], "%Y-%m") #automatically detects which month it is and gets the last full month.

# Convert to Date objects (assume first of the month)
first_date <- as.Date(paste0(first_month, "-01"))
last_date  <- as.Date(paste0(last_month, "-01"))

# Generate sequence by months
month_seq <- seq(first_date, last_date, by = "month")

# Convert back to "YYYY-MM"
month_vec <- format(month_seq, "%Y-%m")

for(month in month_vec) {
  #generate the full filepath for the input and output.
  prefix <- paste0(month,"_charging")
  output_name <- paste0(prefix, ".csv")
  output_path <- file.path(output_root,output_name)
  
  # Get all files in the output directory
  existing_files <- list.files(output_root, full.names = FALSE)

  #check if the file exists
  if(any(startsWith(existing_files, prefix))) {
    cat("Skipping", month, "- found existing file(s):",
        paste(existing_files[startsWith(existing_files, prefix)], collapse = ", "), "\n")
    next

  } 
    
  raw_data_merge_parallel(raw_data_root,month,output_path)
 
}


```


#.csv files now saved to the chosen output directory.

```{r csv conversion to Parquet - If needed}
library(arrow)
library(tools)
library(data.table)

csv_to_parquet <- function(csv_file_paths, dest_folder) {
  # Create destination folder if it doesn't exist
  if (!dir.exists(dest_folder)) {
    dir.create(dest_folder, recursive = TRUE)
  }
  
  for (csv_path in csv_file_paths) {
    # Build output Parquet path
    file_base <- file_path_sans_ext(basename(csv_path))  # strip .csv
    parquet_path <- file.path(dest_folder, paste0(file_base, ".parquet"))
    
    # Skip if Parquet file already exists
    if (file.exists(parquet_path)) {
      message("Skipping (already exists): ", parquet_path)
      next
    }
    
    # Read CSV robustly, filling missing columns with NA
    dt <- fread(csv_path, fill = TRUE)
    
    # Write Parquet
    arrow::write_parquet(dt, parquet_path)
    
    message("Converted: ", csv_path, " -> ", parquet_path)
  }
}

```

```{r}
charging_data_root <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_CSV")
chg_avail_csvs <- list.files(charging_data_root,full.names = TRUE)

output_parquet_path <- file.path("C:/Users/santar_r/Documents/Rudolph_local/Datasets/Swiss-EVChg-Data_Parquet")


csv_to_parquet(chg_avail_csvs, output_parquet_path)
```

#.parquet files now saved to the chosen output directory.